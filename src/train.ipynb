{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD\n",
    "\n",
    "# Part 1\n",
    "\n",
    "## training with VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "#from utils.data_augmentation import ImageGenerator\n",
    "#from utils.train import MultiboxLoss\n",
    "#from utils.train import scheduler\n",
    "#from utils.train import split_data\n",
    "#from models.ssd import SSD300\n",
    "#from utils.boxes import create_prior_boxes\n",
    "#from utils.datasets import DataManager\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, roc_auc_score, \\\n",
    "                            classification_report, precision_recall_curve, average_precision_score\n",
    "\n",
    "# parameters\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "num_classes = 21\n",
    "optimizer = Adam(lr=3e-4)\n",
    "root_prefix = '../datasets/VOCdevkit/VOC2007/'\n",
    "ground_data_prefix = root_prefix + 'Annotations/'\n",
    "image_prefix = root_prefix + 'JPEGImages/'\n",
    "image_shape = (300, 300 ,3)\n",
    "dataset_name = 'VOC2007'\n",
    "weights_path = '../trained_models/weights_SSD300.hdf5'\n",
    "trained_models_path = '../trained_models/model_checkpoints/'\n",
    "trained_models_filename = (trained_models_path +\n",
    "                        'ssd300_weights.{epoch:03d}-{val_loss:.3f}.hdf5')\n",
    "frozen_layers = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "                'conv2_1', 'conv2_2', 'pool2',\n",
    "                'conv3_1', 'conv3_2', 'conv3_3', 'pool3']\n",
    "box_scale_factors = [.1, .1, .2, .2]\n",
    "\n",
    "\n",
    "\n",
    "#voc_classes = ['car','truck','biker','pedestrian','traficlight']\n",
    "#voc_classes = ['Aeroplane', 'Bicycle', 'Bird', 'Boat', 'Bottle',\n",
    "               #'Bus', 'Car', 'Cat', 'Chair', 'Cow', 'Diningtable',\n",
    "               #'Dog', 'Horse','Motorbike', 'Person', 'Pottedplant',\n",
    "               #'Sheep', 'Sofa', 'Train', 'Tvmonitor']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import numpy as np\n",
    "try:\n",
    "    from pycocotools.coco import COCO\n",
    "except ImportError:\n",
    "    COCO = None\n",
    "\n",
    "class XMLParser(object):\n",
    "    \"\"\" Preprocess the VOC2007 xml annotations data.\n",
    "\n",
    "    # TODO: Add background label\n",
    "\n",
    "    # Arguments\n",
    "        data_path: Data path to VOC2007 annotations\n",
    "\n",
    "    # Return\n",
    "        data: Dictionary which keys correspond to the image names\n",
    "        and values are numpy arrays of shape (num_objects, 4 + num_classes)\n",
    "        num_objects refers to the number of objects in that specific image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path, class_names=None, dataset_name='VOC2007'):\n",
    "        self.path_prefix = data_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.class_names = class_names\n",
    "        if self.class_names == None:\n",
    "            self.class_names = get_class_names(self.dataset_name)\n",
    "        self.num_classes = len(self.class_names)\n",
    "        keys = np.arange(self.num_classes)\n",
    "        self.arg_to_class = dict(zip(keys, self.class_names))\n",
    "        self.class_to_arg = {value: key for key, value\n",
    "                             in self.arg_to_class.items()}\n",
    "        self.data = dict()\n",
    "        self.difficult_objects = dict()\n",
    "        self._preprocess_XML()\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.data\n",
    "\n",
    "    def _preprocess_XML(self):\n",
    "        filenames = os.listdir(self.path_prefix)\n",
    "        for filename in filenames:\n",
    "            tree = ElementTree.parse(self.path_prefix + filename)\n",
    "            root = tree.getroot()\n",
    "            bounding_boxes = []\n",
    "            one_hot_classes = []\n",
    "            difficulties = []\n",
    "            size_tree = root.find('size')\n",
    "            width = float(size_tree.find('width').text)\n",
    "            height = float(size_tree.find('height').text)\n",
    "            for object_tree in root.findall('object'):\n",
    "                difficulty = int(object_tree.find('difficult').text)\n",
    "                class_name = object_tree.find('name').text\n",
    "                if class_name in self.class_names:\n",
    "                    one_hot_class = self._to_one_hot(class_name)\n",
    "                    one_hot_classes.append(one_hot_class)\n",
    "                    for bounding_box in object_tree.iter('bndbox'):\n",
    "                        xmin = float(bounding_box.find('xmin').text) / width\n",
    "                        ymin = float(bounding_box.find('ymin').text) / height\n",
    "                        xmax = float(bounding_box.find('xmax').text) / width\n",
    "                        ymax = float(bounding_box.find('ymax').text) / height\n",
    "                    bounding_box = [xmin, ymin, xmax, ymax]\n",
    "                    bounding_boxes.append(bounding_box)\n",
    "                    difficulties.append(difficulty)\n",
    "            if len(one_hot_classes) == 0:\n",
    "                continue\n",
    "            image_name = root.find('filename').text\n",
    "            bounding_boxes = np.asarray(bounding_boxes)\n",
    "            one_hot_classes = np.asarray(one_hot_classes)\n",
    "            image_data = np.hstack((bounding_boxes, one_hot_classes))\n",
    "            if len(bounding_boxes.shape) == 1:\n",
    "                image_data = np.expand_dims(image_data, axis=0)\n",
    "            self.data[image_name] = image_data\n",
    "            self.difficult_objects[image_name] = difficulties\n",
    "\n",
    "    def _to_one_hot(self, class_name):\n",
    "        one_hot_vector = [0] * self.num_classes\n",
    "        class_arg = self.class_to_arg[class_name]\n",
    "        one_hot_vector[class_arg] = 1\n",
    "        return one_hot_vector\n",
    "\n",
    "class COCOParser(object):\n",
    "    def __init__(self, annotations_path, class_names='all'):\n",
    "        self.coco = COCO(annotations_path)\n",
    "        self.class_names = class_names\n",
    "        if self.class_names == 'all':\n",
    "            class_data = self.coco.loadCats(self.coco.getCatIds())\n",
    "            self.class_names = [class_['name'] for class_ in class_data]\n",
    "            coco_ids = [class_['id'] for class_ in class_data]\n",
    "            one_hot_ids = list(range(1, len(coco_ids) + 1))\n",
    "            self.coco_id_to_class_arg = dict(zip(coco_ids, one_hot_ids))\n",
    "            self.class_names = ['background'] + self.class_names\n",
    "            self.num_classes = len(self.class_names)\n",
    "        elif len(self.class_names) > 1:\n",
    "            raise NotImplementedError('Only one or all classes supported')\n",
    "        self.data = dict()\n",
    "\n",
    "    def get_data(self):\n",
    "        self._get_data()\n",
    "        return self.data\n",
    "\n",
    "    def _get_data(self):\n",
    "        image_ids = self.coco.getImgIds()\n",
    "        for image_id in image_ids:\n",
    "            image_data = self.coco.loadImgs(image_id)[0]\n",
    "            image_file_name = image_data['file_name']\n",
    "            width = float(image_data['width'])\n",
    "            height = float(image_data['height'])\n",
    "            annotation_ids = self.coco.getAnnIds(imgIds=image_data['id'])\n",
    "            annotations = self.coco.loadAnns(annotation_ids)\n",
    "            num_objects_in_image = len(annotations)\n",
    "            if num_objects_in_image == 0:\n",
    "                continue\n",
    "            image_ground_truth = []\n",
    "            for object_arg in range(num_objects_in_image):\n",
    "                coco_id = annotations[object_arg]['category_id']\n",
    "                class_arg = self.coco_id_to_class_arg[coco_id]\n",
    "                one_hot_class = self._to_one_hot(class_arg)\n",
    "                coco_coordinates = annotations[object_arg]['bbox']\n",
    "                #print('coco_coordinates:', coco_coordinates)\n",
    "                x_min = (coco_coordinates[0]) #/ width\n",
    "                y_min = (coco_coordinates[1]) #/ height\n",
    "                x_max = (x_min + coco_coordinates[2]) #/ width\n",
    "                y_max = (y_min + coco_coordinates[3]) #/ height\n",
    "                #print('transformed_coordinates:', [x_min, y_min, x_max, y_max])\n",
    "                x_min = x_min / width\n",
    "                y_min = y_min / height\n",
    "                x_max = x_max / width\n",
    "                y_max = y_max / height\n",
    "                #print('normalized_coordinates:', [x_min, y_min, x_max, y_max])\n",
    "                ground_truth = [x_min, y_min, x_max, y_max] + one_hot_class\n",
    "                image_ground_truth.append(ground_truth)\n",
    "            image_ground_truth = np.asarray(image_ground_truth)\n",
    "            if len(image_ground_truth.shape) == 1:\n",
    "                image_ground_truth = np.expand_dims(image_ground_truth, 0)\n",
    "            self.data[image_file_name] = image_ground_truth\n",
    "\n",
    "    def _to_one_hot(self, class_arg):\n",
    "        one_hot_vector = [0] * self.num_classes\n",
    "        one_hot_vector[class_arg] = 1\n",
    "        return one_hot_vector\n",
    "\n",
    "\n",
    "class DataManager(object):\n",
    "    \"\"\"Class for loading VOC2007 and COCO datasets or\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_name='VOC2007', class_names=None,\n",
    "                                    dataset_path_prefix=None,\n",
    "                                    image_prefix=None):\n",
    "\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset_path_prefix = dataset_path_prefix\n",
    "        self.image_prefix = image_prefix\n",
    "        self.class_names = class_names\n",
    "        self.ground_truth_data = None\n",
    "        self.parser = None\n",
    "        if self.dataset_path_prefix == None:\n",
    "            self.dataset_path_prefix = '../datasets/VOCdevkit/VOC2007/Annotations/'\n",
    "        else:\n",
    "            self.dataset_path_prefix = dataset_path_prefix\n",
    "\n",
    "        if self.image_prefix == None:\n",
    "            self.image_prefix = '../datasets/VOCdevkit/VOC2007/JPEGImages/'\n",
    "        else:\n",
    "            self.image_prefix = image_prefix\n",
    "\n",
    "        if self.dataset_name == 'VOC2007':\n",
    "            self._load_VOC2007()\n",
    "        elif self.dataset_name == 'COCO':\n",
    "            self.dataset_path_prefix = ('../datasets/COCO/annotations/' +\n",
    "                                                'instances_train2014.json')\n",
    "            self._load_COCO()\n",
    "        elif self.dataset_name == 'all':\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            raise Exception('Incorrect dataset name:', self.dataset_name)\n",
    "\n",
    "    def get_data(self):\n",
    "        print('Deprecated function use function: load_data instead')\n",
    "        return self.ground_truth_data\n",
    "\n",
    "    def load_data(self):\n",
    "        return self.ground_truth_data\n",
    "\n",
    "    def _load_VOC2007(self):\n",
    "        self.parser = XMLParser(self.dataset_path_prefix, self.class_names)\n",
    "        self.ground_truth_data = self.parser.get_data()\n",
    "        self.class_names = self.parser.class_names\n",
    "        self.arg_to_class = self.parser.arg_to_class\n",
    "        self.class_to_arg = self.parser.class_to_arg\n",
    "\n",
    "    def _load_COCO(self):\n",
    "        self.parser = COCOParser(self.dataset_path_prefix)\n",
    "        self.ground_truth_data = self.parser.get_data()\n",
    "\n",
    "def get_class_names(dataset_name='VOC2007'):\n",
    "    if dataset_name == 'VOC2007':\n",
    "        class_names = ['background','aeroplane', 'bicycle', 'bird', 'boat',\n",
    "                       'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "                       'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "                       'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "    elif dataset_name == 'COCO':\n",
    "        class_names = ['background', 'person', 'bicycle', 'car', 'motorcycle',\n",
    "                        'airplane', 'bus', 'train', 'truck', 'boat',\n",
    "                        'traffic light', 'fire hydrant', 'stop sign',\n",
    "                        'parking meter', 'bench', 'bird', 'cat', 'dog',\n",
    "                        'horse', 'sheep', 'cow', 'elephant', 'bear',\n",
    "                        'zebra', 'giraffe', 'backpack', 'umbrella',\n",
    "                        'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "                        'snowboard', 'sports ball', 'kite', 'baseball bat',\n",
    "                        'baseball glove', 'skateboard', 'surfboard',\n",
    "                        'tennis racket', 'bottle', 'wine glass',\n",
    "                        'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
    "                        'apple', 'sandwich', 'orange', 'broccoli', 'carrot',\n",
    "                        'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "                        'potted plant', 'bed', 'dining table', 'toilet',\n",
    "                        'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "                        'cell phone', 'microwave', 'oven', 'toaster',\n",
    "                        'sink', 'refrigerator', 'book', 'clock', 'vase',\n",
    "                        'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n",
    "    else:\n",
    "        raise Exception('Invalid dataset', dataset_name)\n",
    "    return class_names\n",
    "\n",
    "def get_arg_to_class(class_names):\n",
    "    return dict(zip(list(range(len(class_names))), class_names))\n",
    "\n",
    "def list_files_in_directory(path_name='*'):\n",
    "    return glob.glob(path_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "class MultiboxLoss(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, num_classes, alpha=1.0, neg_pos_ratio=3.0,\n",
    "                 background_id=0, negatives_for_hard=100.0):\n",
    "        self.num_classes = num_classes\n",
    "        self.alpha = alpha\n",
    "        self.neg_pos_ratio = neg_pos_ratio\n",
    "        self.background_id = background_id\n",
    "        self.negatives_for_hard = negatives_for_hard\n",
    "        \n",
    "        '''Multibox loss with some helper functions.\n",
    "        # Arguments\n",
    "        num_classes: Number of classes including background.\n",
    "        alpha: Weight of L1-smooth loss.\n",
    "        neg_pos_ratio: Max ratio of negative to positive boxes in loss.\n",
    "        background_label_id: Id of background label.\n",
    "        negatives_for_hard: Number of negative boxes to consider\n",
    "            it there is no positive boxes in batch.\n",
    "        # References\n",
    "        https://arxiv.org/abs/1512.02325'''\n",
    "        \n",
    "\n",
    "    def _l1_smooth_loss(self, y_true, y_pred):\n",
    "        \n",
    "        \"\"\"Compute L1-smooth loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "            y_pred: Predicted bounding boxes,\n",
    "                tensor of shape (?, num_boxes, 4).\n",
    "        # Returns\n",
    "            l1_loss: L1-smooth loss, tensor of shape (?, num_boxes).\n",
    "        # References\n",
    "            https://arxiv.org/abs/1504.08083\n",
    "        \"\"\"\n",
    "        absolute_value_loss = tf.abs(y_true - y_pred) - 0.5\n",
    "        square_loss = 0.5 * (y_true - y_pred)**2\n",
    "        absolute_value_condition = K.less(absolute_value_loss, 1.0)\n",
    "        l1_smooth_loss = tf.where(absolute_value_condition, square_loss,\n",
    "                                                    absolute_value_loss)\n",
    "        return K.sum(l1_smooth_loss, axis=-1)\n",
    "\n",
    "    def _softmax_loss(self, y_true, y_pred):\n",
    "        \n",
    "        \"\"\"Compute softmax loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, num_classes).\n",
    "        # Returns\n",
    "            softmax_loss: Softmax loss, tensor of shape (?, num_boxes).\n",
    "        \"\"\"\n",
    "        y_pred = K.maximum(K.minimum(y_pred, 1 - 1e-15), 1e-15)\n",
    "        softmax_loss = - K.sum(y_true * K.log(y_pred), axis=-1)\n",
    "        return softmax_loss\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"Compute mutlibox loss.\n",
    "        # Arguments\n",
    "            y_true: Ground truth targets,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8),\n",
    "                priors in ground truth are fictitious,\n",
    "                y_true[:, :, -8] has 1 if prior should be penalized\n",
    "                    or in other words is assigned to some ground truth box,\n",
    "                y_true[:, :, -7:] are all 0.\n",
    "            y_pred: Predicted logits,\n",
    "                tensor of shape (?, num_boxes, 4 + num_classes + 8).\n",
    "        # Returns\n",
    "            loss: Loss for prediction, tensor of shape (?,).\n",
    "        \"\"\"\n",
    "        batch_size = K.shape(y_true)[0]\n",
    "        num_prior_boxes = K.cast(K.shape(y_true)[1], 'float')\n",
    "\n",
    "        y_pred_localization = y_pred[:, :, :4]\n",
    "        y_true_localization = y_true[:, :, :4]\n",
    "        y_pred_classification = y_pred[:, :, 4:(4 + self.num_classes)]\n",
    "        y_true_classification = y_true[:, :, 4:(4 + self.num_classes)]\n",
    "\n",
    "        localization_loss = self._l1_smooth_loss(y_true_localization,\n",
    "                                                 y_pred_localization)\n",
    "        classification_loss = self._softmax_loss(y_true_classification,\n",
    "                                                 y_pred_classification)\n",
    "\n",
    "        int_positive_mask = 1 - y_true[:, :, 4 + self.background_id]\n",
    "        num_positives = tf.reduce_sum(int_positive_mask, axis=-1)\n",
    "        positive_localization_losses = (localization_loss * int_positive_mask)\n",
    "        positive_classification_losses = (classification_loss *\n",
    "                                          int_positive_mask)\n",
    "        positive_classification_loss = K.sum(positive_classification_losses, 1)\n",
    "        positive_localization_loss = K.sum(positive_localization_losses, 1)\n",
    "\n",
    "        num_negatives_1 = self.neg_pos_ratio * num_positives\n",
    "        num_negatives_2 = num_prior_boxes - num_positives\n",
    "        num_negatives = tf.minimum(num_negatives_1, num_negatives_2)\n",
    "\n",
    "        num_positive_mask = tf.greater(num_negatives, 0)\n",
    "        has_a_positive = tf.to_float(tf.reduce_any(num_positive_mask))\n",
    "        num_negatives = tf.concat([num_negatives,\n",
    "                        [(1 - has_a_positive) * self.negatives_for_hard]], 0)\n",
    "        num_positive_mask = tf.greater(num_negatives, 0)\n",
    "        num_neg_batch = tf.reduce_min(tf.boolean_mask(num_negatives,\n",
    "                                                num_positive_mask))\n",
    "        num_neg_batch = tf.to_int32(num_neg_batch)\n",
    "\n",
    "        pred_class_values = K.max(y_pred_classification[:, :, 1:], axis=2)\n",
    "        int_negatives_mask = y_true[:, :, 4 + self.background_id]\n",
    "        pred_negative_class_values = pred_class_values * int_negatives_mask\n",
    "        top_k_negative_indices = tf.nn.top_k(pred_negative_class_values,\n",
    "                                                    k=num_neg_batch)[1]\n",
    "\n",
    "        batch_indices = K.expand_dims(K.arange(0, batch_size), 1)\n",
    "        batch_indices = K.tile(batch_indices, (1, num_neg_batch))\n",
    "        batch_indices = K.flatten(batch_indices) * K.cast(num_prior_boxes,\n",
    "                                                                'int32')\n",
    "        full_indices = batch_indices + K.flatten(top_k_negative_indices)\n",
    "\n",
    "        negative_classification_loss = K.gather(K.flatten(classification_loss),\n",
    "                                                                full_indices)\n",
    "        negative_classification_loss = K.reshape(negative_classification_loss,\n",
    "                                                    [batch_size, num_neg_batch])\n",
    "        negative_classification_loss = K.sum(negative_classification_loss, 1)\n",
    "\n",
    "        # loss is sum of positives and negatives\n",
    "        total_loss = (positive_classification_loss +\n",
    "                      negative_classification_loss)\n",
    "        num_prior_boxes_per_batch = (num_positives +\n",
    "                                     K.cast(num_neg_batch, 'float'))\n",
    "        total_loss = total_loss / num_prior_boxes_per_batch\n",
    "        num_positives = tf.where(K.not_equal(num_positives, 0), num_positives,\n",
    "                                                   K.ones_like(num_positives))\n",
    "        positive_localization_loss = self.alpha * positive_classification_loss\n",
    "        positive_localization_loss = positive_localization_loss / num_positives\n",
    "        total_loss = total_loss + positive_localization_loss\n",
    "        return total_loss\n",
    "\n",
    "def scheduler(epoch, decay=0.9, base_learning_rate=3e-4):\n",
    "    return base_learning_rate * decay**(epoch)\n",
    "\n",
    "def split_data(ground_truths, training_ratio=.8):\n",
    "    ground_truth_keys = sorted(ground_truths.keys())\n",
    "    num_train = int(round(training_ratio * len(ground_truth_keys)))\n",
    "    train_keys = ground_truth_keys[:num_train]\n",
    "    validation_keys = ground_truth_keys[num_train:]\n",
    "    return train_keys, validation_keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth_data sample "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n"
     ]
    }
   ],
   "source": [
    "# loading and splitting data\n",
    "data_manager = DataManager(dataset_name)\n",
    "\n",
    "ground_truth_data = data_manager.load_data()\n",
    "\n",
    " \n",
    "\n",
    "train_keys, validation_keys = split_data(ground_truth_data, training_ratio=.8)\n",
    "\n",
    "print('ground_truth_data sample',ground_truth_data )\n",
    "print('train_keys', len(train_keys))\n",
    "print('validation_keys', len(validation_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.engine.topology import InputSpec\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "class Normalize(Layer):\n",
    "    \"\"\"Normalization layer as described in ParseNet paper.\n",
    "\n",
    "    # Arguments\n",
    "        scale: Default feature scale.\n",
    "\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "\n",
    "    # Output shape\n",
    "        Same as input\n",
    "\n",
    "    # References\n",
    "        http://cs.unc.edu/~wliu/papers/parsenet.pdf\n",
    "\n",
    "    #TODO\n",
    "        Add possibility to have one scale for all features.\n",
    "    \"\"\"\n",
    "    def __init__(self, scale, **kwargs):\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            self.axis = 3\n",
    "        else:\n",
    "            self.axis = 1\n",
    "        self.scale = scale\n",
    "        super(Normalize, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        shape = (input_shape[self.axis],)\n",
    "        init_gamma = self.scale * np.ones(shape)\n",
    "        self.gamma = K.variable(init_gamma, name='{}_gamma'.format(self.name))\n",
    "        self.trainable_weights = [self.gamma]\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        output = K.l2_normalize(x, self.axis)\n",
    "        output *= self.gamma\n",
    "        return output\n",
    "\n",
    "\n",
    "class PriorBox(Layer):\n",
    "    \"\"\"Generate the prior boxes of designated sizes and aspect ratios.\n",
    "\n",
    "    # Arguments\n",
    "        img_size: Size of the input image as tuple (w, h).\n",
    "        min_size: Minimum box size in pixels.\n",
    "        max_size: Maximum box size in pixels.\n",
    "        aspect_ratios: List of aspect ratios of boxes.\n",
    "        flip: Whether to consider reverse aspect ratios.\n",
    "        variances: List of variances for x, y, w, h.\n",
    "        clip: Whether to clip the prior's coordinates\n",
    "            such that they are within [0, 1].\n",
    "\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "\n",
    "    # Output shape\n",
    "        3D tensor with shape:\n",
    "        (samples, num_boxes, 8)\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "\n",
    "    #TODO\n",
    "        Add possibility not to have variances.\n",
    "        Add Theano support\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size, min_size, max_size=None, aspect_ratios=None,\n",
    "                 flip=True, variances=[0.1], clip=True, **kwargs):\n",
    "        if K.image_dim_ordering() == 'tf':\n",
    "            self.waxis = 2\n",
    "            self.haxis = 1\n",
    "        else:\n",
    "            self.waxis = 3\n",
    "            self.haxis = 2\n",
    "        self.img_size = img_size\n",
    "        if min_size <= 0:\n",
    "            raise Exception('min_size must be positive.')\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "        self.aspect_ratios = [1.0]\n",
    "        if max_size:\n",
    "            if max_size < min_size:\n",
    "                raise Exception('max_size must be greater than min_size.')\n",
    "            self.aspect_ratios.append(1.0)\n",
    "        if aspect_ratios:\n",
    "            for ar in aspect_ratios:\n",
    "                if ar in self.aspect_ratios:\n",
    "                    continue\n",
    "                self.aspect_ratios.append(ar)\n",
    "                if flip:\n",
    "                    self.aspect_ratios.append(1.0 / ar)\n",
    "        self.variances = np.array(variances)\n",
    "        self.clip = True\n",
    "        super(PriorBox, self).__init__(**kwargs)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        num_priors_ = len(self.aspect_ratios)\n",
    "        layer_width = input_shape[self.waxis]\n",
    "        layer_height = input_shape[self.haxis]\n",
    "        num_boxes = num_priors_ * layer_width * layer_height\n",
    "        return input_shape[0], num_boxes, 8\n",
    "\n",
    "    # support for Keras 2.0\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self.get_output_shape_for(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if hasattr(x, '_keras_shape'):\n",
    "            input_shape = x._keras_shape\n",
    "        elif hasattr(K, 'int_shape'):\n",
    "            input_shape = K.int_shape(x)\n",
    "        layer_width = input_shape[self.waxis]\n",
    "        layer_height = input_shape[self.haxis]\n",
    "        img_width = self.img_size[0]\n",
    "        img_height = self.img_size[1]\n",
    "        # define prior boxes shapes\n",
    "        box_widths = []\n",
    "        box_heights = []\n",
    "        for ar in self.aspect_ratios:\n",
    "            if ar == 1 and len(box_widths) == 0:\n",
    "                box_widths.append(self.min_size)\n",
    "                box_heights.append(self.min_size)\n",
    "            elif ar == 1 and len(box_widths) > 0:\n",
    "                box_widths.append(np.sqrt(self.min_size * self.max_size))\n",
    "                box_heights.append(np.sqrt(self.min_size * self.max_size))\n",
    "            elif ar != 1:\n",
    "                box_widths.append(self.min_size * np.sqrt(ar))\n",
    "                box_heights.append(self.min_size / np.sqrt(ar))\n",
    "        box_widths = 0.5 * np.array(box_widths)\n",
    "        box_heights = 0.5 * np.array(box_heights)\n",
    "        # define centers of prior boxes\n",
    "        step_x = img_width / layer_width\n",
    "        step_y = img_height / layer_height\n",
    "        linx = np.linspace(0.5 * step_x, img_width - 0.5 * step_x,\n",
    "                           layer_width)\n",
    "        liny = np.linspace(0.5 * step_y, img_height - 0.5 * step_y,\n",
    "                           layer_height)\n",
    "        centers_x, centers_y = np.meshgrid(linx, liny)\n",
    "        centers_x = centers_x.reshape(-1, 1)\n",
    "        centers_y = centers_y.reshape(-1, 1)\n",
    "        # define xmin, ymin, xmax, ymax of prior boxes\n",
    "        num_priors_ = len(self.aspect_ratios)\n",
    "        prior_boxes = np.concatenate((centers_x, centers_y), axis=1)\n",
    "        prior_boxes = np.tile(prior_boxes, (1, 2 * num_priors_))\n",
    "        prior_boxes[:, ::4] -= box_widths\n",
    "        prior_boxes[:, 1::4] -= box_heights\n",
    "        prior_boxes[:, 2::4] += box_widths\n",
    "        prior_boxes[:, 3::4] += box_heights\n",
    "        prior_boxes[:, ::2] /= img_width\n",
    "        prior_boxes[:, 1::2] /= img_height\n",
    "        prior_boxes = prior_boxes.reshape(-1, 4)\n",
    "        if self.clip:\n",
    "            prior_boxes = np.minimum(np.maximum(prior_boxes, 0.0), 1.0)\n",
    "        # define variances\n",
    "        num_boxes = len(prior_boxes)\n",
    "        if len(self.variances) == 1:\n",
    "            variances = np.ones((num_boxes, 4)) * self.variances[0]\n",
    "        elif len(self.variances) == 4:\n",
    "            variances = np.tile(self.variances, (num_boxes, 1))\n",
    "        else:\n",
    "            raise Exception('Must provide one or four variances.')\n",
    "        prior_boxes = np.concatenate((prior_boxes, variances), axis=1)\n",
    "        prior_boxes_tensor = K.expand_dims(K.variable(prior_boxes), 0)\n",
    "        pattern = [K.shape(x)[0], 1, 1]\n",
    "        prior_boxes_tensor = K.tile(prior_boxes_tensor, pattern)\n",
    "        return prior_boxes_tensor\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        if K.backend() == 'tensorflow':\n",
    "            pattern = [tf.shape(x)[0], 1, 1]\n",
    "            prior_boxes_tensor = tf.tile(prior_boxes_tensor, pattern)\n",
    "        elif K.backend() == 'theano':\n",
    "            #TODO\n",
    "            pass\n",
    "        return prior_boxes_tensor\n",
    "        \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\"\"\"Keras implementation of SSD.\"\"\"\\n\\nimport keras.backend as K\\nfrom keras.layers import Activation\\nfrom keras.layers import AtrousConvolution2D\\nfrom keras.layers import Convolution2D\\nfrom keras.layers import Dense\\nfrom keras.layers import Flatten\\nfrom keras.layers import GlobalAveragePooling2D\\nfrom keras.layers import Input\\nfrom keras.layers import MaxPooling2D\\nfrom keras.layers import merge\\nfrom keras.layers import Reshape\\nfrom keras.layers import ZeroPadding2D\\nfrom keras.models import Model\\n\\nfrom ssd_layers import Normalize\\nfrom ssd_layers import PriorBox\\n\\n\\ndef SSD300(input_shape, num_classes=21):\\n    \"\"\"SSD300 architecture.\\n\\n    # Arguments\\n        input_shape: Shape of the input image,\\n            expected to be either (300, 300, 3) or (3, 300, 300)(not tested).\\n        num_classes: Number of classes including background.\\n\\n    # References\\n        https://arxiv.org/abs/1512.02325\\n    \"\"\"\\n    net = {}\\n    # Block 1\\n    input_tensor = input_tensor = Input(shape=input_shape)\\n    img_size = (input_shape[1], input_shape[0])\\n    net[\\'input\\'] = input_tensor\\n    net[\\'conv1_1\\'] = Convolution2D(64, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv1_1\\')(net[\\'input\\'])\\n    net[\\'conv1_2\\'] = Convolution2D(64, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv1_2\\')(net[\\'conv1_1\\'])\\n    net[\\'pool1\\'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode=\\'same\\',\\n                                name=\\'pool1\\')(net[\\'conv1_2\\'])\\n    # Block 2\\n    net[\\'conv2_1\\'] = Convolution2D(128, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv2_1\\')(net[\\'pool1\\'])\\n    net[\\'conv2_2\\'] = Convolution2D(128, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv2_2\\')(net[\\'conv2_1\\'])\\n    net[\\'pool2\\'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode=\\'same\\',\\n                                name=\\'pool2\\')(net[\\'conv2_2\\'])\\n    # Block 3\\n    net[\\'conv3_1\\'] = Convolution2D(256, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv3_1\\')(net[\\'pool2\\'])\\n    net[\\'conv3_2\\'] = Convolution2D(256, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv3_2\\')(net[\\'conv3_1\\'])\\n    net[\\'conv3_3\\'] = Convolution2D(256, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv3_3\\')(net[\\'conv3_2\\'])\\n    net[\\'pool3\\'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode=\\'same\\',\\n                                name=\\'pool3\\')(net[\\'conv3_3\\'])\\n    # Block 4\\n    net[\\'conv4_1\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv4_1\\')(net[\\'pool3\\'])\\n    net[\\'conv4_2\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv4_2\\')(net[\\'conv4_1\\'])\\n    net[\\'conv4_3\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv4_3\\')(net[\\'conv4_2\\'])\\n    net[\\'pool4\\'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode=\\'same\\',\\n                                name=\\'pool4\\')(net[\\'conv4_3\\'])\\n    # Block 5\\n    net[\\'conv5_1\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv5_1\\')(net[\\'pool4\\'])\\n    net[\\'conv5_2\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv5_2\\')(net[\\'conv5_1\\'])\\n    net[\\'conv5_3\\'] = Convolution2D(512, 3, 3,\\n                                   activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv5_3\\')(net[\\'conv5_2\\'])\\n    net[\\'pool5\\'] = MaxPooling2D((3, 3), strides=(1, 1), border_mode=\\'same\\',\\n                                name=\\'pool5\\')(net[\\'conv5_3\\'])\\n    # FC6\\n    net[\\'fc6\\'] = AtrousConvolution2D(1024, 3, 3, atrous_rate=(6, 6),\\n                                     activation=\\'relu\\', border_mode=\\'same\\',\\n                                     name=\\'fc6\\')(net[\\'pool5\\'])\\n    # x = Dropout(0.5, name=\\'drop6\\')(x)\\n    # FC7\\n    net[\\'fc7\\'] = Convolution2D(1024, 1, 1, activation=\\'relu\\',\\n                               border_mode=\\'same\\', name=\\'fc7\\')(net[\\'fc6\\'])\\n    # x = Dropout(0.5, name=\\'drop7\\')(x)\\n    # Block 6\\n    net[\\'conv6_1\\'] = Convolution2D(256, 1, 1, activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv6_1\\')(net[\\'fc7\\'])\\n    net[\\'conv6_2\\'] = Convolution2D(512, 3, 3, subsample=(2, 2),\\n                                   activation=\\'relu\\', border_mode=\\'same\\',\\n                                   name=\\'conv6_2\\')(net[\\'conv6_1\\'])\\n    # Block 7\\n    net[\\'conv7_1\\'] = Convolution2D(128, 1, 1, activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv7_1\\')(net[\\'conv6_2\\'])\\n    net[\\'conv7_2\\'] = ZeroPadding2D()(net[\\'conv7_1\\'])\\n    net[\\'conv7_2\\'] = Convolution2D(256, 3, 3, subsample=(2, 2),\\n                                   activation=\\'relu\\', border_mode=\\'valid\\',\\n                                   name=\\'conv7_2\\')(net[\\'conv7_2\\'])\\n    # Block 8\\n    net[\\'conv8_1\\'] = Convolution2D(128, 1, 1, activation=\\'relu\\',\\n                                   border_mode=\\'same\\',\\n                                   name=\\'conv8_1\\')(net[\\'conv7_2\\'])\\n    net[\\'conv8_2\\'] = Convolution2D(256, 3, 3, subsample=(2, 2),\\n                                   activation=\\'relu\\', border_mode=\\'same\\',\\n                                   name=\\'conv8_2\\')(net[\\'conv8_1\\'])\\n    # Last Pool\\n    net[\\'pool6\\'] = GlobalAveragePooling2D(name=\\'pool6\\')(net[\\'conv8_2\\'])\\n    # Prediction from conv4_3\\n    net[\\'conv4_3_norm\\'] = Normalize(20, name=\\'conv4_3_norm\\')(net[\\'conv4_3\\'])\\n    num_priors = 3\\n    x = Convolution2D(num_priors * 4, 3, 3, border_mode=\\'same\\',\\n                      name=\\'conv4_3_norm_mbox_loc\\')(net[\\'conv4_3_norm\\'])\\n    net[\\'conv4_3_norm_mbox_loc\\'] = x\\n    flatten = Flatten(name=\\'conv4_3_norm_mbox_loc_flat\\')\\n    net[\\'conv4_3_norm_mbox_loc_flat\\'] = flatten(net[\\'conv4_3_norm_mbox_loc\\'])\\n    name = \\'conv4_3_norm_mbox_conf\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode=\\'same\\',\\n                      name=name)(net[\\'conv4_3_norm\\'])\\n    net[\\'conv4_3_norm_mbox_conf\\'] = x\\n    flatten = Flatten(name=\\'conv4_3_norm_mbox_conf_flat\\')\\n    net[\\'conv4_3_norm_mbox_conf_flat\\'] = flatten(net[\\'conv4_3_norm_mbox_conf\\'])\\n    priorbox = PriorBox(img_size, 30.0, aspect_ratios=[2],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'conv4_3_norm_mbox_priorbox\\')\\n    net[\\'conv4_3_norm_mbox_priorbox\\'] = priorbox(net[\\'conv4_3_norm\\'])\\n    # Prediction from fc7\\n    num_priors = 6\\n    net[\\'fc7_mbox_loc\\'] = Convolution2D(num_priors * 4, 3, 3,\\n                                        border_mode=\\'same\\',\\n                                        name=\\'fc7_mbox_loc\\')(net[\\'fc7\\'])\\n    flatten = Flatten(name=\\'fc7_mbox_loc_flat\\')\\n    net[\\'fc7_mbox_loc_flat\\'] = flatten(net[\\'fc7_mbox_loc\\'])\\n    name = \\'fc7_mbox_conf\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    net[\\'fc7_mbox_conf\\'] = Convolution2D(num_priors * num_classes, 3, 3,\\n                                         border_mode=\\'same\\',\\n                                         name=name)(net[\\'fc7\\'])\\n    flatten = Flatten(name=\\'fc7_mbox_conf_flat\\')\\n    net[\\'fc7_mbox_conf_flat\\'] = flatten(net[\\'fc7_mbox_conf\\'])\\n    priorbox = PriorBox(img_size, 60.0, max_size=114.0, aspect_ratios=[2, 3],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'fc7_mbox_priorbox\\')\\n    net[\\'fc7_mbox_priorbox\\'] = priorbox(net[\\'fc7\\'])\\n    # Prediction from conv6_2\\n    num_priors = 6\\n    x = Convolution2D(num_priors * 4, 3, 3, border_mode=\\'same\\',\\n                      name=\\'conv6_2_mbox_loc\\')(net[\\'conv6_2\\'])\\n    net[\\'conv6_2_mbox_loc\\'] = x\\n    flatten = Flatten(name=\\'conv6_2_mbox_loc_flat\\')\\n    net[\\'conv6_2_mbox_loc_flat\\'] = flatten(net[\\'conv6_2_mbox_loc\\'])\\n    name = \\'conv6_2_mbox_conf\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode=\\'same\\',\\n                      name=name)(net[\\'conv6_2\\'])\\n    net[\\'conv6_2_mbox_conf\\'] = x\\n    flatten = Flatten(name=\\'conv6_2_mbox_conf_flat\\')\\n    net[\\'conv6_2_mbox_conf_flat\\'] = flatten(net[\\'conv6_2_mbox_conf\\'])\\n    priorbox = PriorBox(img_size, 114.0, max_size=168.0, aspect_ratios=[2, 3],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'conv6_2_mbox_priorbox\\')\\n    net[\\'conv6_2_mbox_priorbox\\'] = priorbox(net[\\'conv6_2\\'])\\n    # Prediction from conv7_2\\n    num_priors = 6\\n    x = Convolution2D(num_priors * 4, 3, 3, border_mode=\\'same\\',\\n                      name=\\'conv7_2_mbox_loc\\')(net[\\'conv7_2\\'])\\n    net[\\'conv7_2_mbox_loc\\'] = x\\n    flatten = Flatten(name=\\'conv7_2_mbox_loc_flat\\')\\n    net[\\'conv7_2_mbox_loc_flat\\'] = flatten(net[\\'conv7_2_mbox_loc\\'])\\n    name = \\'conv7_2_mbox_conf\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode=\\'same\\',\\n                      name=name)(net[\\'conv7_2\\'])\\n    net[\\'conv7_2_mbox_conf\\'] = x\\n    flatten = Flatten(name=\\'conv7_2_mbox_conf_flat\\')\\n    net[\\'conv7_2_mbox_conf_flat\\'] = flatten(net[\\'conv7_2_mbox_conf\\'])\\n    priorbox = PriorBox(img_size, 168.0, max_size=222.0, aspect_ratios=[2, 3],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'conv7_2_mbox_priorbox\\')\\n    net[\\'conv7_2_mbox_priorbox\\'] = priorbox(net[\\'conv7_2\\'])\\n    # Prediction from conv8_2\\n    num_priors = 6\\n    x = Convolution2D(num_priors * 4, 3, 3, border_mode=\\'same\\',\\n                      name=\\'conv8_2_mbox_loc\\')(net[\\'conv8_2\\'])\\n    net[\\'conv8_2_mbox_loc\\'] = x\\n    flatten = Flatten(name=\\'conv8_2_mbox_loc_flat\\')\\n    net[\\'conv8_2_mbox_loc_flat\\'] = flatten(net[\\'conv8_2_mbox_loc\\'])\\n    name = \\'conv8_2_mbox_conf\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode=\\'same\\',\\n                      name=name)(net[\\'conv8_2\\'])\\n    net[\\'conv8_2_mbox_conf\\'] = x\\n    flatten = Flatten(name=\\'conv8_2_mbox_conf_flat\\')\\n    net[\\'conv8_2_mbox_conf_flat\\'] = flatten(net[\\'conv8_2_mbox_conf\\'])\\n    priorbox = PriorBox(img_size, 222.0, max_size=276.0, aspect_ratios=[2, 3],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'conv8_2_mbox_priorbox\\')\\n    net[\\'conv8_2_mbox_priorbox\\'] = priorbox(net[\\'conv8_2\\'])\\n    # Prediction from pool6\\n    num_priors = 6\\n    x = Dense(num_priors * 4, name=\\'pool6_mbox_loc_flat\\')(net[\\'pool6\\'])\\n    net[\\'pool6_mbox_loc_flat\\'] = x\\n    name = \\'pool6_mbox_conf_flat\\'\\n    if num_classes != 21:\\n        name += \\'_{}\\'.format(num_classes)\\n    x = Dense(num_priors * num_classes, name=name)(net[\\'pool6\\'])\\n    net[\\'pool6_mbox_conf_flat\\'] = x\\n    priorbox = PriorBox(img_size, 276.0, max_size=330.0, aspect_ratios=[2, 3],\\n                        variances=[0.1, 0.1, 0.2, 0.2],\\n                        name=\\'pool6_mbox_priorbox\\')\\n    if K.image_dim_ordering() == \\'tf\\':\\n        target_shape = (1, 1, 256)\\n    else:\\n        target_shape = (256, 1, 1)\\n    net[\\'pool6_reshaped\\'] = Reshape(target_shape,\\n                                    name=\\'pool6_reshaped\\')(net[\\'pool6\\'])\\n    net[\\'pool6_mbox_priorbox\\'] = priorbox(net[\\'pool6_reshaped\\'])\\n    # Gather all predictions\\n    net[\\'mbox_loc\\'] = merge([net[\\'conv4_3_norm_mbox_loc_flat\\'],\\n                             net[\\'fc7_mbox_loc_flat\\'],\\n                             net[\\'conv6_2_mbox_loc_flat\\'],\\n                             net[\\'conv7_2_mbox_loc_flat\\'],\\n                             net[\\'conv8_2_mbox_loc_flat\\'],\\n                             net[\\'pool6_mbox_loc_flat\\']],\\n                            mode=\\'concat\\', concat_axis=1, name=\\'mbox_loc\\')\\n    net[\\'mbox_conf\\'] = merge([net[\\'conv4_3_norm_mbox_conf_flat\\'],\\n                              net[\\'fc7_mbox_conf_flat\\'],\\n                              net[\\'conv6_2_mbox_conf_flat\\'],\\n                              net[\\'conv7_2_mbox_conf_flat\\'],\\n                              net[\\'conv8_2_mbox_conf_flat\\'],\\n                              net[\\'pool6_mbox_conf_flat\\']],\\n                             mode=\\'concat\\', concat_axis=1, name=\\'mbox_conf\\')\\n    net[\\'mbox_priorbox\\'] = merge([net[\\'conv4_3_norm_mbox_priorbox\\'],\\n                                  net[\\'fc7_mbox_priorbox\\'],\\n                                  net[\\'conv6_2_mbox_priorbox\\'],\\n                                  net[\\'conv7_2_mbox_priorbox\\'],\\n                                  net[\\'conv8_2_mbox_priorbox\\'],\\n                                  net[\\'pool6_mbox_priorbox\\']],\\n                                 mode=\\'concat\\', concat_axis=1,\\n                                 name=\\'mbox_priorbox\\')\\n    if hasattr(net[\\'mbox_loc\\'], \\'_keras_shape\\'):\\n        num_boxes = net[\\'mbox_loc\\']._keras_shape[-1] // 4\\n    elif hasattr(net[\\'mbox_loc\\'], \\'int_shape\\'):\\n        num_boxes = K.int_shape(net[\\'mbox_loc\\'])[-1] // 4\\n    net[\\'mbox_loc\\'] = Reshape((num_boxes, 4),\\n                              name=\\'mbox_loc_final\\')(net[\\'mbox_loc\\'])\\n    net[\\'mbox_conf\\'] = Reshape((num_boxes, num_classes),\\n                               name=\\'mbox_conf_logits\\')(net[\\'mbox_conf\\'])\\n    net[\\'mbox_conf\\'] = Activation(\\'softmax\\',\\n                                  name=\\'mbox_conf_final\\')(net[\\'mbox_conf\\'])\\n    net[\\'predictions\\'] = merge([net[\\'mbox_loc\\'],\\n                               net[\\'mbox_conf\\'],\\n                               net[\\'mbox_priorbox\\']],\\n                               mode=\\'concat\\', concat_axis=2,\\n                               name=\\'predictions\\')\\n    model = Model(net[\\'input\\'], net[\\'predictions\\'])\\n    return model\\n\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "#from .layers import Normalize\n",
    "#from .layers import PriorBox\n",
    "\n",
    "\n",
    "def SSD300(input_shape=(300, 300, 3), num_classes=21, weights_path=None,\n",
    "                                                    frozen_layers=None):\n",
    "    \"\"\"SSD300 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: Shape of the input image,\n",
    "            expected to be either (300, 300, 3) or (3, 300, 300)(not tested).\n",
    "        num_classes: Number of classes including background.\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    \"\"\"\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    # Block 1\n",
    "    conv1_1 = Conv2D(64, (3, 3),\n",
    "                     name='conv1_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(input_layer)\n",
    "\n",
    "    conv1_2 = Conv2D(64, (3, 3),\n",
    "                     name='conv1_2',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv1_1)\n",
    "    pool1 = MaxPooling2D(name='pool1',\n",
    "                         pool_size=(2, 2),\n",
    "                         strides=(2, 2),\n",
    "                         padding='same', )(conv1_2)\n",
    "\n",
    "    # Block 2\n",
    "    conv2_1 = Conv2D(128, (3, 3),\n",
    "                     name='conv2_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(pool1)\n",
    "    conv2_2 = Conv2D(128, (3, 3),\n",
    "                     name='conv2_2',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv2_1)\n",
    "    pool2 = MaxPooling2D(name='pool2',\n",
    "                         pool_size=(2, 2),\n",
    "                         strides=(2, 2),\n",
    "                         padding='same')(conv2_2)\n",
    "\n",
    "    # Block 3\n",
    "    conv3_1 = Conv2D(256, (3, 3),\n",
    "                     name='conv3_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(pool2)\n",
    "    conv3_2 = Conv2D(256, (3, 3),\n",
    "                     name='conv3_2',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv3_1)\n",
    "    conv3_3 = Conv2D(256, (3, 3),\n",
    "                     name='conv3_3',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv3_2)\n",
    "    pool3 = MaxPooling2D(name='pool3',\n",
    "                         pool_size=(2, 2),\n",
    "                         strides=(2, 2),\n",
    "                         padding='same')(conv3_3)\n",
    "\n",
    "    # Block 4\n",
    "    conv4_1 = Conv2D(512, (3, 3),\n",
    "                     name='conv4_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(pool3)\n",
    "    conv4_2 = Conv2D(512, (3, 3),\n",
    "                     name='conv4_2',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv4_1)\n",
    "    conv4_3 = Conv2D(512, (3, 3),\n",
    "                     name='conv4_3',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv4_2)\n",
    "    pool4 = MaxPooling2D(name='pool4',\n",
    "                         pool_size=(2, 2),\n",
    "                         strides=(2, 2),\n",
    "                         padding='same')(conv4_3)\n",
    "\n",
    "    # Block 5\n",
    "    conv5_1 = Conv2D(512, (3, 3),\n",
    "                     name='conv5_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(pool4)\n",
    "    conv5_2 = Conv2D(512, (3, 3),\n",
    "                     name='conv5_2',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv5_1)\n",
    "    conv5_3 = Conv2D(512, (3, 3),\n",
    "                     name='conv5_3',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv5_2)\n",
    "    pool5 = MaxPooling2D(name='pool5',\n",
    "                         pool_size=(3, 3),\n",
    "                         strides=(1, 1),\n",
    "                         padding='same')(conv5_3)\n",
    "\n",
    "    # FC6\n",
    "    fc6 = Conv2D(1024, (3, 3),\n",
    "                 name='fc6',\n",
    "                 dilation_rate=(6, 6),\n",
    "                 padding='same',\n",
    "                 activation='relu'\n",
    "                 )(pool5)\n",
    "\n",
    "    # x = Dropout(0.5, name='drop6')(x)\n",
    "    # FC7\n",
    "    fc7 = Conv2D(1024, (1, 1),\n",
    "                 name='fc7',\n",
    "                 padding='same',\n",
    "                 activation='relu'\n",
    "                 )(fc6)\n",
    "    # x = Dropout(0.5, name='drop7')(x)\n",
    "\n",
    "    # Block 6\n",
    "    conv6_1 = Conv2D(256, (1, 1),\n",
    "                     name='conv6_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(fc7)\n",
    "    conv6_2 = Conv2D(512, (3, 3),\n",
    "                     name='conv6_2',\n",
    "                     strides=(2, 2),\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv6_1)\n",
    "\n",
    "    # Block 7\n",
    "    conv7_1 = Conv2D(128, (1, 1),\n",
    "                     name='conv7_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv6_2)\n",
    "    conv7_1z = ZeroPadding2D(name='conv7_1z')(conv7_1)\n",
    "    conv7_2 = Conv2D(256, (3, 3),\n",
    "                     name='conv7_2',\n",
    "                     padding='valid',\n",
    "                     strides=(2, 2),\n",
    "                     activation='relu')(conv7_1z)\n",
    "\n",
    "    # Block 8\n",
    "    conv8_1 = Conv2D(128, (1, 1),\n",
    "                     name='conv8_1',\n",
    "                     padding='same',\n",
    "                     activation='relu')(conv7_2)\n",
    "    conv8_2 = Conv2D(256, (3, 3),\n",
    "                     name='conv8_2',\n",
    "                     padding='same',\n",
    "                     strides=(2, 2),\n",
    "                     activation='relu')(conv8_1)\n",
    "\n",
    "    # Last Pool\n",
    "    pool6 = GlobalAveragePooling2D(name='pool6')(conv8_2)\n",
    "\n",
    "    # Prediction from conv4_3\n",
    "    num_priors = 3\n",
    "    img_size = (input_shape[1], input_shape[0])\n",
    "    name = 'conv4_3_norm_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "\n",
    "    conv4_3_norm = Normalize(20, name='conv4_3_norm')(conv4_3)\n",
    "    conv4_3_norm_mbox_loc = Conv2D(num_priors * 4, (3, 3),\n",
    "                                   name='conv4_3_norm_mbox_loc',\n",
    "                                   padding='same')(conv4_3_norm)\n",
    "    conv4_3_norm_mbox_loc_flat = Flatten(name='conv4_3_norm_mbox_loc_flat')(conv4_3_norm_mbox_loc)\n",
    "    conv4_3_norm_mbox_conf = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                                    name=name,\n",
    "                                    padding='same')(conv4_3_norm)\n",
    "    conv4_3_norm_mbox_conf_flat = Flatten(name='conv4_3_norm_mbox_conf_flat')(conv4_3_norm_mbox_conf)\n",
    "    conv4_3_norm_mbox_priorbox = PriorBox(img_size, 30.0,\n",
    "                                          name='conv4_3_norm_mbox_priorbox',\n",
    "                                          aspect_ratios=[2],\n",
    "                                          variances=[0.1, 0.1, 0.2, 0.2])(conv4_3_norm)\n",
    "\n",
    "    # Prediction from fc7\n",
    "    num_priors = 6\n",
    "    name = 'fc7_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    fc7_mbox_conf = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                           padding='same',\n",
    "                           name=name)(fc7)\n",
    "    fc7_mbox_conf_flat = Flatten(name='fc7_mbox_conf_flat')(fc7_mbox_conf)\n",
    "\n",
    "    fc7_mbox_loc = Conv2D(num_priors * 4, (3, 3),\n",
    "                          name='fc7_mbox_loc',\n",
    "                          padding='same')(fc7)\n",
    "    fc7_mbox_loc_flat = Flatten(name='fc7_mbox_loc_flat')(fc7_mbox_loc)\n",
    "    fc7_mbox_priorbox = PriorBox(img_size, 60.0,\n",
    "                                 name='fc7_mbox_priorbox',\n",
    "                                 max_size=114.0,\n",
    "                                 aspect_ratios=[2, 3],\n",
    "                                 variances=[0.1, 0.1, 0.2, 0.2]\n",
    "                                 )(fc7)\n",
    "\n",
    "    # Prediction from conv6_2\n",
    "    num_priors = 6\n",
    "    name = 'conv6_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    conv6_2_mbox_conf = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                               padding='same',\n",
    "                               name=name)(conv6_2)\n",
    "    conv6_2_mbox_conf_flat = Flatten(name='conv6_2_mbox_conf_flat')(conv6_2_mbox_conf)\n",
    "    conv6_2_mbox_loc = Conv2D(num_priors * 4, (3, 3,),\n",
    "                              name='conv6_2_mbox_loc',\n",
    "                              padding='same')(conv6_2)\n",
    "    conv6_2_mbox_loc_flat = Flatten(name='conv6_2_mbox_loc_flat')(conv6_2_mbox_loc)\n",
    "    conv6_2_mbox_priorbox = PriorBox(img_size, 114.0,\n",
    "                                     max_size=168.0,\n",
    "                                     aspect_ratios=[2, 3],\n",
    "                                     variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                                     name='conv6_2_mbox_priorbox')(conv6_2)\n",
    "    # Prediction from conv7_2\n",
    "    num_priors = 6\n",
    "    name = 'conv7_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    conv7_2_mbox_conf = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                               padding='same',\n",
    "                               name=name)(conv7_2)\n",
    "    conv7_2_mbox_conf_flat = Flatten(name='conv7_2_mbox_conf_flat')(conv7_2_mbox_conf)\n",
    "    conv7_2_mbox_loc = Conv2D(num_priors * 4, (3, 3),\n",
    "                              padding='same',\n",
    "                              name='conv7_2_mbox_loc')(conv7_2)\n",
    "    conv7_2_mbox_loc_flat = Flatten(name='conv7_2_mbox_loc_flat')(conv7_2_mbox_loc)\n",
    "    conv7_2_mbox_priorbox = PriorBox(img_size, 168.0,\n",
    "                                     max_size=222.0,\n",
    "                                     aspect_ratios=[2, 3],\n",
    "                                     variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                                     name='conv7_2_mbox_priorbox')(conv7_2)\n",
    "    # Prediction from conv8_2\n",
    "    num_priors = 6\n",
    "    name = 'conv8_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    conv8_2_mbox_conf = Conv2D(num_priors * num_classes, (3, 3),\n",
    "                               padding='same',\n",
    "                               name=name)(conv8_2)\n",
    "    conv8_2_mbox_conf_flat = Flatten(name='conv8_2_mbox_conf_flat')(conv8_2_mbox_conf)\n",
    "    conv8_2_mbox_loc = Conv2D(num_priors * 4, (3, 3),\n",
    "                              padding='same',\n",
    "                              name='conv8_2_mbox_loc')(conv8_2)\n",
    "    conv8_2_mbox_loc_flat = Flatten(name='conv8_2_mbox_loc_flat')(conv8_2_mbox_loc)\n",
    "    conv8_2_mbox_priorbox = PriorBox(img_size, 222.0,\n",
    "                                     max_size=276.0,\n",
    "                                     aspect_ratios=[2, 3],\n",
    "                                     variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                                     name='conv8_2_mbox_priorbox')(conv8_2)\n",
    "\n",
    "    # Prediction from pool6\n",
    "    num_priors = 6\n",
    "    name = 'pool6_mbox_conf_flat'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        target_shape = (1, 1, 256)\n",
    "    else:\n",
    "        target_shape = (256, 1, 1)\n",
    "    pool6_mbox_loc_flat = Dense(num_priors * 4, name='pool6_mbox_loc_flat')(pool6)\n",
    "    pool6_mbox_conf_flat = Dense(num_priors * num_classes, name=name)(pool6)\n",
    "    pool6_reshaped = Reshape(target_shape,\n",
    "                             name='pool6_reshaped')(pool6)\n",
    "    pool6_mbox_priorbox = PriorBox(img_size, 276.0, max_size=330.0, aspect_ratios=[2, 3],\n",
    "                                   variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                                   name='pool6_mbox_priorbox')(pool6_reshaped)\n",
    "    # Gather all predictions\n",
    "    mbox_loc = concatenate([conv4_3_norm_mbox_loc_flat,\n",
    "                            fc7_mbox_loc_flat,\n",
    "                            conv6_2_mbox_loc_flat,\n",
    "                            conv7_2_mbox_loc_flat,\n",
    "                            conv8_2_mbox_loc_flat,\n",
    "                            pool6_mbox_loc_flat],\n",
    "                           axis=1,\n",
    "                           name='mbox_loc')\n",
    "    mbox_conf = concatenate([conv4_3_norm_mbox_conf_flat,\n",
    "                             fc7_mbox_conf_flat,\n",
    "                             conv6_2_mbox_conf_flat,\n",
    "                             conv7_2_mbox_conf_flat,\n",
    "                             conv8_2_mbox_conf_flat,\n",
    "                             pool6_mbox_conf_flat],\n",
    "                            axis=1,\n",
    "                            name='mbox_conf')\n",
    "    mbox_priorbox = concatenate([conv4_3_norm_mbox_priorbox,\n",
    "                                 fc7_mbox_priorbox,\n",
    "                                 conv6_2_mbox_priorbox,\n",
    "                                 conv7_2_mbox_priorbox,\n",
    "                                 conv8_2_mbox_priorbox,\n",
    "                                 pool6_mbox_priorbox],\n",
    "                                axis=1,\n",
    "                                name='mbox_priorbox')\n",
    "    if hasattr(mbox_loc, '_keras_shape'):\n",
    "        num_boxes = mbox_loc._keras_shape[-1] // 4\n",
    "    elif hasattr(mbox_loc, 'int_shape'):\n",
    "        num_boxes = K.int_shape(mbox_loc)[-1] // 4\n",
    "    mbox_loc = Reshape((num_boxes, 4),\n",
    "                       name='mbox_loc_final')(mbox_loc)\n",
    "    mbox_conf = Reshape((num_boxes, num_classes),\n",
    "                        name='mbox_conf_logits')(mbox_conf)\n",
    "    mbox_conf = Activation('softmax',\n",
    "                           name='mbox_conf_final')(mbox_conf)\n",
    "    predictions = concatenate([mbox_loc,\n",
    "                               mbox_conf,\n",
    "                               mbox_priorbox],\n",
    "                              axis=2,\n",
    "                              name='predictions')\n",
    "    model = Model(inputs=input_layer, outputs=predictions)\n",
    "\n",
    "    if weights_path is not None:\n",
    "        model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    if frozen_layers is not None:\n",
    "        for layer in model.layers:\n",
    "            if layer.name in frozen_layers:\n",
    "                layer.trainable = False\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "\"\"\"Keras implementation of SSD.\"\"\"\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Activation\n",
    "from keras.layers import AtrousConvolution2D\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import merge\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.models import Model\n",
    "\n",
    "from ssd_layers import Normalize\n",
    "from ssd_layers import PriorBox\n",
    "\n",
    "\n",
    "def SSD300(input_shape, num_classes=21):\n",
    "    \"\"\"SSD300 architecture.\n",
    "\n",
    "    # Arguments\n",
    "        input_shape: Shape of the input image,\n",
    "            expected to be either (300, 300, 3) or (3, 300, 300)(not tested).\n",
    "        num_classes: Number of classes including background.\n",
    "\n",
    "    # References\n",
    "        https://arxiv.org/abs/1512.02325\n",
    "    \"\"\"\n",
    "    net = {}\n",
    "    # Block 1\n",
    "    input_tensor = input_tensor = Input(shape=input_shape)\n",
    "    img_size = (input_shape[1], input_shape[0])\n",
    "    net['input'] = input_tensor\n",
    "    net['conv1_1'] = Convolution2D(64, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv1_1')(net['input'])\n",
    "    net['conv1_2'] = Convolution2D(64, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv1_2')(net['conv1_1'])\n",
    "    net['pool1'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode='same',\n",
    "                                name='pool1')(net['conv1_2'])\n",
    "    # Block 2\n",
    "    net['conv2_1'] = Convolution2D(128, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv2_1')(net['pool1'])\n",
    "    net['conv2_2'] = Convolution2D(128, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv2_2')(net['conv2_1'])\n",
    "    net['pool2'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode='same',\n",
    "                                name='pool2')(net['conv2_2'])\n",
    "    # Block 3\n",
    "    net['conv3_1'] = Convolution2D(256, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv3_1')(net['pool2'])\n",
    "    net['conv3_2'] = Convolution2D(256, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv3_2')(net['conv3_1'])\n",
    "    net['conv3_3'] = Convolution2D(256, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv3_3')(net['conv3_2'])\n",
    "    net['pool3'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode='same',\n",
    "                                name='pool3')(net['conv3_3'])\n",
    "    # Block 4\n",
    "    net['conv4_1'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv4_1')(net['pool3'])\n",
    "    net['conv4_2'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv4_2')(net['conv4_1'])\n",
    "    net['conv4_3'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv4_3')(net['conv4_2'])\n",
    "    net['pool4'] = MaxPooling2D((2, 2), strides=(2, 2), border_mode='same',\n",
    "                                name='pool4')(net['conv4_3'])\n",
    "    # Block 5\n",
    "    net['conv5_1'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv5_1')(net['pool4'])\n",
    "    net['conv5_2'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv5_2')(net['conv5_1'])\n",
    "    net['conv5_3'] = Convolution2D(512, 3, 3,\n",
    "                                   activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv5_3')(net['conv5_2'])\n",
    "    net['pool5'] = MaxPooling2D((3, 3), strides=(1, 1), border_mode='same',\n",
    "                                name='pool5')(net['conv5_3'])\n",
    "    # FC6\n",
    "    net['fc6'] = AtrousConvolution2D(1024, 3, 3, atrous_rate=(6, 6),\n",
    "                                     activation='relu', border_mode='same',\n",
    "                                     name='fc6')(net['pool5'])\n",
    "    # x = Dropout(0.5, name='drop6')(x)\n",
    "    # FC7\n",
    "    net['fc7'] = Convolution2D(1024, 1, 1, activation='relu',\n",
    "                               border_mode='same', name='fc7')(net['fc6'])\n",
    "    # x = Dropout(0.5, name='drop7')(x)\n",
    "    # Block 6\n",
    "    net['conv6_1'] = Convolution2D(256, 1, 1, activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv6_1')(net['fc7'])\n",
    "    net['conv6_2'] = Convolution2D(512, 3, 3, subsample=(2, 2),\n",
    "                                   activation='relu', border_mode='same',\n",
    "                                   name='conv6_2')(net['conv6_1'])\n",
    "    # Block 7\n",
    "    net['conv7_1'] = Convolution2D(128, 1, 1, activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv7_1')(net['conv6_2'])\n",
    "    net['conv7_2'] = ZeroPadding2D()(net['conv7_1'])\n",
    "    net['conv7_2'] = Convolution2D(256, 3, 3, subsample=(2, 2),\n",
    "                                   activation='relu', border_mode='valid',\n",
    "                                   name='conv7_2')(net['conv7_2'])\n",
    "    # Block 8\n",
    "    net['conv8_1'] = Convolution2D(128, 1, 1, activation='relu',\n",
    "                                   border_mode='same',\n",
    "                                   name='conv8_1')(net['conv7_2'])\n",
    "    net['conv8_2'] = Convolution2D(256, 3, 3, subsample=(2, 2),\n",
    "                                   activation='relu', border_mode='same',\n",
    "                                   name='conv8_2')(net['conv8_1'])\n",
    "    # Last Pool\n",
    "    net['pool6'] = GlobalAveragePooling2D(name='pool6')(net['conv8_2'])\n",
    "    # Prediction from conv4_3\n",
    "    net['conv4_3_norm'] = Normalize(20, name='conv4_3_norm')(net['conv4_3'])\n",
    "    num_priors = 3\n",
    "    x = Convolution2D(num_priors * 4, 3, 3, border_mode='same',\n",
    "                      name='conv4_3_norm_mbox_loc')(net['conv4_3_norm'])\n",
    "    net['conv4_3_norm_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv4_3_norm_mbox_loc_flat')\n",
    "    net['conv4_3_norm_mbox_loc_flat'] = flatten(net['conv4_3_norm_mbox_loc'])\n",
    "    name = 'conv4_3_norm_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode='same',\n",
    "                      name=name)(net['conv4_3_norm'])\n",
    "    net['conv4_3_norm_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv4_3_norm_mbox_conf_flat')\n",
    "    net['conv4_3_norm_mbox_conf_flat'] = flatten(net['conv4_3_norm_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, 30.0, aspect_ratios=[2],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv4_3_norm_mbox_priorbox')\n",
    "    net['conv4_3_norm_mbox_priorbox'] = priorbox(net['conv4_3_norm'])\n",
    "    # Prediction from fc7\n",
    "    num_priors = 6\n",
    "    net['fc7_mbox_loc'] = Convolution2D(num_priors * 4, 3, 3,\n",
    "                                        border_mode='same',\n",
    "                                        name='fc7_mbox_loc')(net['fc7'])\n",
    "    flatten = Flatten(name='fc7_mbox_loc_flat')\n",
    "    net['fc7_mbox_loc_flat'] = flatten(net['fc7_mbox_loc'])\n",
    "    name = 'fc7_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    net['fc7_mbox_conf'] = Convolution2D(num_priors * num_classes, 3, 3,\n",
    "                                         border_mode='same',\n",
    "                                         name=name)(net['fc7'])\n",
    "    flatten = Flatten(name='fc7_mbox_conf_flat')\n",
    "    net['fc7_mbox_conf_flat'] = flatten(net['fc7_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, 60.0, max_size=114.0, aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='fc7_mbox_priorbox')\n",
    "    net['fc7_mbox_priorbox'] = priorbox(net['fc7'])\n",
    "    # Prediction from conv6_2\n",
    "    num_priors = 6\n",
    "    x = Convolution2D(num_priors * 4, 3, 3, border_mode='same',\n",
    "                      name='conv6_2_mbox_loc')(net['conv6_2'])\n",
    "    net['conv6_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv6_2_mbox_loc_flat')\n",
    "    net['conv6_2_mbox_loc_flat'] = flatten(net['conv6_2_mbox_loc'])\n",
    "    name = 'conv6_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode='same',\n",
    "                      name=name)(net['conv6_2'])\n",
    "    net['conv6_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv6_2_mbox_conf_flat')\n",
    "    net['conv6_2_mbox_conf_flat'] = flatten(net['conv6_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, 114.0, max_size=168.0, aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv6_2_mbox_priorbox')\n",
    "    net['conv6_2_mbox_priorbox'] = priorbox(net['conv6_2'])\n",
    "    # Prediction from conv7_2\n",
    "    num_priors = 6\n",
    "    x = Convolution2D(num_priors * 4, 3, 3, border_mode='same',\n",
    "                      name='conv7_2_mbox_loc')(net['conv7_2'])\n",
    "    net['conv7_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv7_2_mbox_loc_flat')\n",
    "    net['conv7_2_mbox_loc_flat'] = flatten(net['conv7_2_mbox_loc'])\n",
    "    name = 'conv7_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode='same',\n",
    "                      name=name)(net['conv7_2'])\n",
    "    net['conv7_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv7_2_mbox_conf_flat')\n",
    "    net['conv7_2_mbox_conf_flat'] = flatten(net['conv7_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, 168.0, max_size=222.0, aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv7_2_mbox_priorbox')\n",
    "    net['conv7_2_mbox_priorbox'] = priorbox(net['conv7_2'])\n",
    "    # Prediction from conv8_2\n",
    "    num_priors = 6\n",
    "    x = Convolution2D(num_priors * 4, 3, 3, border_mode='same',\n",
    "                      name='conv8_2_mbox_loc')(net['conv8_2'])\n",
    "    net['conv8_2_mbox_loc'] = x\n",
    "    flatten = Flatten(name='conv8_2_mbox_loc_flat')\n",
    "    net['conv8_2_mbox_loc_flat'] = flatten(net['conv8_2_mbox_loc'])\n",
    "    name = 'conv8_2_mbox_conf'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Convolution2D(num_priors * num_classes, 3, 3, border_mode='same',\n",
    "                      name=name)(net['conv8_2'])\n",
    "    net['conv8_2_mbox_conf'] = x\n",
    "    flatten = Flatten(name='conv8_2_mbox_conf_flat')\n",
    "    net['conv8_2_mbox_conf_flat'] = flatten(net['conv8_2_mbox_conf'])\n",
    "    priorbox = PriorBox(img_size, 222.0, max_size=276.0, aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='conv8_2_mbox_priorbox')\n",
    "    net['conv8_2_mbox_priorbox'] = priorbox(net['conv8_2'])\n",
    "    # Prediction from pool6\n",
    "    num_priors = 6\n",
    "    x = Dense(num_priors * 4, name='pool6_mbox_loc_flat')(net['pool6'])\n",
    "    net['pool6_mbox_loc_flat'] = x\n",
    "    name = 'pool6_mbox_conf_flat'\n",
    "    if num_classes != 21:\n",
    "        name += '_{}'.format(num_classes)\n",
    "    x = Dense(num_priors * num_classes, name=name)(net['pool6'])\n",
    "    net['pool6_mbox_conf_flat'] = x\n",
    "    priorbox = PriorBox(img_size, 276.0, max_size=330.0, aspect_ratios=[2, 3],\n",
    "                        variances=[0.1, 0.1, 0.2, 0.2],\n",
    "                        name='pool6_mbox_priorbox')\n",
    "    if K.image_dim_ordering() == 'tf':\n",
    "        target_shape = (1, 1, 256)\n",
    "    else:\n",
    "        target_shape = (256, 1, 1)\n",
    "    net['pool6_reshaped'] = Reshape(target_shape,\n",
    "                                    name='pool6_reshaped')(net['pool6'])\n",
    "    net['pool6_mbox_priorbox'] = priorbox(net['pool6_reshaped'])\n",
    "    # Gather all predictions\n",
    "    net['mbox_loc'] = merge([net['conv4_3_norm_mbox_loc_flat'],\n",
    "                             net['fc7_mbox_loc_flat'],\n",
    "                             net['conv6_2_mbox_loc_flat'],\n",
    "                             net['conv7_2_mbox_loc_flat'],\n",
    "                             net['conv8_2_mbox_loc_flat'],\n",
    "                             net['pool6_mbox_loc_flat']],\n",
    "                            mode='concat', concat_axis=1, name='mbox_loc')\n",
    "    net['mbox_conf'] = merge([net['conv4_3_norm_mbox_conf_flat'],\n",
    "                              net['fc7_mbox_conf_flat'],\n",
    "                              net['conv6_2_mbox_conf_flat'],\n",
    "                              net['conv7_2_mbox_conf_flat'],\n",
    "                              net['conv8_2_mbox_conf_flat'],\n",
    "                              net['pool6_mbox_conf_flat']],\n",
    "                             mode='concat', concat_axis=1, name='mbox_conf')\n",
    "    net['mbox_priorbox'] = merge([net['conv4_3_norm_mbox_priorbox'],\n",
    "                                  net['fc7_mbox_priorbox'],\n",
    "                                  net['conv6_2_mbox_priorbox'],\n",
    "                                  net['conv7_2_mbox_priorbox'],\n",
    "                                  net['conv8_2_mbox_priorbox'],\n",
    "                                  net['pool6_mbox_priorbox']],\n",
    "                                 mode='concat', concat_axis=1,\n",
    "                                 name='mbox_priorbox')\n",
    "    if hasattr(net['mbox_loc'], '_keras_shape'):\n",
    "        num_boxes = net['mbox_loc']._keras_shape[-1] // 4\n",
    "    elif hasattr(net['mbox_loc'], 'int_shape'):\n",
    "        num_boxes = K.int_shape(net['mbox_loc'])[-1] // 4\n",
    "    net['mbox_loc'] = Reshape((num_boxes, 4),\n",
    "                              name='mbox_loc_final')(net['mbox_loc'])\n",
    "    net['mbox_conf'] = Reshape((num_boxes, num_classes),\n",
    "                               name='mbox_conf_logits')(net['mbox_conf'])\n",
    "    net['mbox_conf'] = Activation('softmax',\n",
    "                                  name='mbox_conf_final')(net['mbox_conf'])\n",
    "    net['predictions'] = merge([net['mbox_loc'],\n",
    "                               net['mbox_conf'],\n",
    "                               net['mbox_priorbox']],\n",
    "                               mode='concat', concat_axis=2,\n",
    "                               name='predictions')\n",
    "    model = Model(net['input'], net['predictions'])\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating model\n",
    "from keras import metrics\n",
    "model = SSD300(image_shape, num_classes, weights_path, frozen_layers)\n",
    "multibox_loss = MultiboxLoss(num_classes, neg_pos_ratio=2.0).compute_loss\n",
    "model.compile(optimizer, loss=multibox_loss, metrics=['acc'])#,metrics.mae, metrics.categorical_accuracy])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as pil_image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image as keras_image_preprocessor\n",
    "\n",
    "def preprocess_images(image_array):\n",
    "    return preprocess_input(image_array)\n",
    "\n",
    "def load_image(image_path, target_size=None, grayscale=False):\n",
    "    image = keras_image_preprocessor.load_img(image_path,\n",
    "                                                grayscale,\n",
    "                                    target_size=target_size)\n",
    "    return keras_image_preprocessor.img_to_array(image)\n",
    "\n",
    "def preprocess_images2(image_array, backend='tensorflow'):\n",
    "    if backend == 'tensorflow':\n",
    "        # 'RGB'->'BGR'\n",
    "        image_array = image_array[:, :, :, ::-1]\n",
    "    # Zero-center by mean pixel\n",
    "    image_array[:, 0, :, :] -= 103.939\n",
    "    image_array[:, 1, :, :] -= 116.779\n",
    "    image_array[:, 2, :, :] -= 123.68\n",
    "    return image_array\n",
    "\n",
    "def load_image2(path, target_size=None):\n",
    "    image = load_pil_image(path)\n",
    "    image = resize_image(image, target_size)\n",
    "    return image_to_array(image)\n",
    "\n",
    "def load_pil_image(path):\n",
    "    image = pil_image.open(path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return image\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    height_width_tuple = (target_size[1], target_size[0])\n",
    "    if image.size != height_width_tuple:\n",
    "        image = image.resize(height_width_tuple)\n",
    "    return image\n",
    "\n",
    "def resize_image_array(image_array, target_size):\n",
    "    image = array_to_image(image_array)\n",
    "    image = resize_image(image, target_size)\n",
    "    return image_to_array(image)\n",
    "\n",
    "def image_to_array(image, backend='tensorflow'):\n",
    "    image_array = np.asarray(image, dtype='float32')\n",
    "    return image_array\n",
    "\n",
    "def array_to_image(image_array, backend='tensorflow'):\n",
    "    image_array = image_array.astype('uint8')\n",
    "    return pil_image.fromarray(image_array, 'RGB')\n",
    "\n",
    "def get_image_size(path):\n",
    "    image = pil_image.open(path)\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    return image.size[::-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_intersection_over_union(box_data, prior_boxes):\n",
    "    \"\"\"Calculate intersection over union of box_data with respect to\n",
    "    prior_boxes.\n",
    "\n",
    "    Arguments:\n",
    "        ground_truth_data: numpy array with shape (4) indicating x_min, y_min,\n",
    "        x_max and y_max coordinates of the bounding box.\n",
    "        prior_boxes: numpy array with shape (num_boxes, 4).\n",
    "\n",
    "    Returns:\n",
    "        intersections_over_unions: numpy array with shape (num_boxes) which\n",
    "        corresponds to the intersection over unions of box_data with respect\n",
    "        to all prior_boxes.\n",
    "    \"\"\"\n",
    "    x_min = box_data[0]\n",
    "    y_min = box_data[1]\n",
    "    x_max = box_data[2]\n",
    "    y_max = box_data[3]\n",
    "    prior_boxes_x_min = prior_boxes[:, 0]\n",
    "    prior_boxes_y_min = prior_boxes[:, 1]\n",
    "    prior_boxes_x_max = prior_boxes[:, 2]\n",
    "    prior_boxes_y_max = prior_boxes[:, 3]\n",
    "    # calculating the intersection\n",
    "    intersections_x_min = np.maximum(prior_boxes_x_min, x_min)\n",
    "    intersections_y_min = np.maximum(prior_boxes_y_min, y_min)\n",
    "    intersections_x_max = np.minimum(prior_boxes_x_max, x_max)\n",
    "    intersections_y_max = np.minimum(prior_boxes_y_max, y_max)\n",
    "    intersected_widths = intersections_x_max - intersections_x_min\n",
    "    intersected_heights = intersections_y_max - intersections_y_min\n",
    "    intersected_widths = np.maximum(intersected_widths, 0)\n",
    "    intersected_heights = np.maximum(intersected_heights, 0)\n",
    "    intersections = intersected_widths * intersected_heights\n",
    "    # calculating the union\n",
    "    prior_box_widths = prior_boxes_x_max - prior_boxes_x_min\n",
    "    prior_box_heights = prior_boxes_y_max - prior_boxes_y_min\n",
    "    prior_box_areas = prior_box_widths * prior_box_heights\n",
    "    box_width = x_max - x_min\n",
    "    box_height = y_max - y_min\n",
    "    ground_truth_area = box_width * box_height\n",
    "    unions = prior_box_areas + ground_truth_area - intersections\n",
    "    intersection_over_union = intersections / unions\n",
    "    return intersection_over_union\n",
    "\n",
    "def regress_boxes(assigned_prior_boxes, ground_truth_box, box_scale_factors):\n",
    "    \"\"\"Regress assigned_prior_boxes to ground_truth_box as mentioned in\n",
    "    Faster-RCNN and Single-shot Multi-box Detector papers.\n",
    "\n",
    "    Arguments:\n",
    "        assigned_prior_boxes: numpy array with shape (num_assigned_priors, 4)\n",
    "        indicating x_min, y_min, x_max and y_max for every prior box.\n",
    "        ground_truth_box: numpy array with shape (4) indicating\n",
    "        x_min, y_min, x_max and y_max of the ground truth box.\n",
    "        box_scale_factors: numpy array with shape (4) containing\n",
    "        the values for scaling the localization gradient.\n",
    "        (https://github.com/weiliu89/caffe/issues/155)\n",
    "\n",
    "    Returns:\n",
    "        regressed_boxes: numpy array with shape (num_assigned_boxes)\n",
    "        which correspond to the regressed values of all\n",
    "        assigned_prior_boxes to the ground_truth_box\n",
    "    \"\"\"\n",
    "    d_box_values = assigned_prior_boxes\n",
    "    d_box_coordinates = d_box_values[:, 0:4]\n",
    "    d_x_min = d_box_coordinates[:, 0]\n",
    "    d_y_min = d_box_coordinates[:, 1]\n",
    "    d_x_max = d_box_coordinates[:, 2]\n",
    "    d_y_max = d_box_coordinates[:, 3]\n",
    "    d_center_x = 0.5 * (d_x_min + d_x_max)\n",
    "    d_center_y = 0.5 * (d_y_min + d_y_max)\n",
    "    d_width =  d_x_max - d_x_min\n",
    "    d_height = d_y_max - d_y_min\n",
    "\n",
    "    g_box_coordinates = ground_truth_box\n",
    "    g_x_min = g_box_coordinates[0]\n",
    "    g_y_min = g_box_coordinates[1]\n",
    "    g_x_max = g_box_coordinates[2]\n",
    "    g_y_max = g_box_coordinates[3]\n",
    "    g_width =  g_x_max - g_x_min\n",
    "    g_height = g_y_max - g_y_min\n",
    "    g_center_x = 0.5 * (g_x_min + g_x_max)\n",
    "    g_center_y = 0.5 * (g_y_min + g_y_max)\n",
    "\n",
    "    scale_center_x = box_scale_factors[0]\n",
    "    scale_center_y = box_scale_factors[1]\n",
    "    scale_width = box_scale_factors[2]\n",
    "    scale_height = box_scale_factors[3]\n",
    "\n",
    "    g_hat_center_x = (g_center_x - d_center_x) / (d_width * scale_center_x)\n",
    "    g_hat_center_y = (g_center_y - d_center_y) / (d_height * scale_center_y)\n",
    "    g_hat_width  = np.log(g_width  / d_width) / scale_width\n",
    "    g_hat_height = np.log(g_height / d_height) / scale_height\n",
    "    regressed_boxes = np.concatenate([g_hat_center_x.reshape(-1, 1),\n",
    "                                    g_hat_center_y.reshape(-1, 1),\n",
    "                                    g_hat_width.reshape(-1, 1),\n",
    "                                    g_hat_height.reshape(-1, 1)],\n",
    "                                    axis=1)\n",
    "    return regressed_boxes\n",
    "\n",
    "def decode_boxes(predicted_boxes, prior_boxes, box_scale_factors):\n",
    "    \"\"\"Decode from regressed coordinates in predicted_boxes\n",
    "    to box_coordinates in prior_boxes by applying the inverse function of the\n",
    "    regress_boxes function.\n",
    "\n",
    "    Arguments:\n",
    "        predicted_boxes: numpy array with shape (num_assigned_priors, 4)\n",
    "        indicating x_min, y_min, x_max and y_max for every prior box.\n",
    "        prior_boxes: numpy array with shape (4) indicating\n",
    "        x_min, y_min, x_max and y_max of the ground truth box.\n",
    "        box_scale_factors: numpy array with shape (num_boxes, 4)\n",
    "        Which represents a scaling of the localization gradient.\n",
    "        (https://github.com/weiliu89/caffe/issues/155)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: if the number of predicted_boxes is not the same as\n",
    "        the number of prior boxes.\n",
    "\n",
    "    Returns:\n",
    "        decoded_boxes: numpy array with shape (num_predicted_boxes, 4) or\n",
    "        (num_prior_boxes, 4 + num_clases) which correspond\n",
    "        to the decoded box coordinates of all prior_boxes.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(predicted_boxes) != len(prior_boxes):\n",
    "        raise ValueError(\n",
    "                'Mismatch between predicted_boxes and prior_boxes length')\n",
    "\n",
    "    prior_x_min = prior_boxes[:, 0]\n",
    "    prior_y_min = prior_boxes[:, 1]\n",
    "    prior_x_max = prior_boxes[:, 2]\n",
    "    prior_y_max = prior_boxes[:, 3]\n",
    "\n",
    "    prior_width = prior_x_max - prior_x_min\n",
    "    prior_height = prior_y_max - prior_y_min\n",
    "    prior_center_x = 0.5 * (prior_x_max + prior_x_min)\n",
    "    prior_center_y = 0.5 * (prior_y_max + prior_y_min)\n",
    "\n",
    "    pred_center_x = predicted_boxes[:, 0]\n",
    "    pred_center_y = predicted_boxes[:, 1]\n",
    "    pred_width = predicted_boxes[:, 2]\n",
    "    pred_height = predicted_boxes[:, 3]\n",
    "\n",
    "    scale_center_x = box_scale_factors[0]\n",
    "    scale_center_y = box_scale_factors[1]\n",
    "    scale_width = box_scale_factors[2]\n",
    "    scale_height = box_scale_factors[3]\n",
    "\n",
    "    decoded_center_x = pred_center_x * prior_width * scale_center_x\n",
    "    decoded_center_x = decoded_center_x + prior_center_x\n",
    "    decoded_center_y = pred_center_y * prior_height * scale_center_y\n",
    "    decoded_center_y = decoded_center_y + prior_center_y\n",
    "\n",
    "    decoded_width = np.exp(pred_width * scale_width)\n",
    "    decoded_width = decoded_width * prior_width\n",
    "    decoded_height = np.exp(pred_height * scale_height)\n",
    "    decoded_height = decoded_height * prior_height\n",
    "\n",
    "    decoded_x_min = decoded_center_x - (0.5 * decoded_width)\n",
    "    decoded_y_min = decoded_center_y - (0.5 * decoded_height)\n",
    "    decoded_x_max = decoded_center_x + (0.5 * decoded_width)\n",
    "    decoded_y_max = decoded_center_y + (0.5 * decoded_height)\n",
    "\n",
    "    decoded_boxes = np.concatenate((decoded_x_min[:, None],\n",
    "                                  decoded_y_min[:, None],\n",
    "                                  decoded_x_max[:, None],\n",
    "                                  decoded_y_max[:, None]), axis=-1)\n",
    "    decoded_boxes = np.clip(decoded_boxes, 0.0, 1.0)\n",
    "    if predicted_boxes.shape[1] > 4:\n",
    "        decoded_boxes = np.concatenate([decoded_boxes,\n",
    "                            predicted_boxes[:, 4:]], axis=-1)\n",
    "    return decoded_boxes\n",
    "\n",
    "def assign_prior_boxes_to_ground_truth(ground_truth_box, prior_boxes,\n",
    "                            box_scale_factors, regress=True,\n",
    "                            overlap_threshold=.5, return_iou=True):\n",
    "    \"\"\" Assigns and regresses prior boxes to a single ground_truth_box\n",
    "    data sample.\n",
    "    TODO: Change this function so that it does not regress the boxes\n",
    "    automatically. It should only assign them but not regress them!\n",
    "    Arguments:\n",
    "        prior_boxes: numpy array with shape (num_prior_boxes, 4)\n",
    "        indicating x_min, y_min, x_max and y_max for every prior box.\n",
    "        ground_truth_box: numpy array with shape (4) indicating\n",
    "        x_min, y_min, x_max and y_max of the ground truth box.\n",
    "        box_scale_factors: numpy array with shape (num_boxes, 4)\n",
    "        Which represents a scaling of the localization gradient.\n",
    "        (https://github.com/weiliu89/caffe/issues/155)\n",
    "\n",
    "    Returns:\n",
    "        regressed_boxes: numpy array with shape (num_assigned_boxes)\n",
    "        which correspond to the regressed values of all\n",
    "        assigned_prior_boxes to the ground_truth_box\n",
    "    \"\"\"\n",
    "    ious = calculate_intersection_over_union(ground_truth_box, prior_boxes)\n",
    "    regressed_boxes = np.zeros((len(prior_boxes), 4 + return_iou))\n",
    "    assign_mask = ious > overlap_threshold\n",
    "    if not assign_mask.any():\n",
    "        assign_mask[ious.argmax()] = True\n",
    "    if return_iou:\n",
    "        regressed_boxes[:, -1][assign_mask] = ious[assign_mask]\n",
    "    assigned_prior_boxes = prior_boxes[assign_mask]\n",
    "    if regress:\n",
    "        assigned_regressed_priors = regress_boxes(assigned_prior_boxes,\n",
    "                                ground_truth_box, box_scale_factors)\n",
    "        regressed_boxes[assign_mask, 0:4] = assigned_regressed_priors\n",
    "        return regressed_boxes.ravel()\n",
    "    else:\n",
    "        regressed_boxes[assign_mask, 0:4] = assigned_prior_boxes[:, 0:4]\n",
    "        return regressed_boxes.ravel()\n",
    "\n",
    "def assign_prior_boxes(prior_boxes, ground_truth_data, num_classes,\n",
    "                        box_scale_factors, regress=True,\n",
    "                        overlap_threshold=.5, background_id=0):\n",
    "    \"\"\" Assign and regress prior boxes to all ground truth samples.\n",
    "    Arguments:\n",
    "        prior_boxes: numpy array with shape (num_prior_boxes, 4)\n",
    "        indicating x_min, y_min, x_max and y_max for every prior box.\n",
    "        ground_truth_data: numpy array with shape (num_samples, 4)\n",
    "        indicating x_min, y_min, x_max and y_max of the ground truth box.\n",
    "        box_scale_factors: numpy array with shape (num_boxes, 4)\n",
    "        Which represents a scaling of the localization gradient.\n",
    "        (https://github.com/weiliu89/caffe/issues/155)\n",
    "\n",
    "    Returns:\n",
    "        assignments: numpy array with shape\n",
    "        (num_samples, 4 + num_classes + 8)\n",
    "        which correspond to the regressed values of all\n",
    "        assigned_prior_boxes to the ground_truth_box\n",
    "    \"\"\"\n",
    "    #assignments = np.zeros((len(prior_boxes), 4 + num_classes + 8))\n",
    "    assignments = np.zeros((len(prior_boxes), 4 + num_classes))\n",
    "    assignments[:, 4 + background_id] = 1.0\n",
    "    num_objects_in_image = len(ground_truth_data)\n",
    "    if num_objects_in_image == 0:\n",
    "        return assignments\n",
    "    encoded_boxes = np.apply_along_axis(assign_prior_boxes_to_ground_truth, 1,\n",
    "                                        ground_truth_data[:, :4], prior_boxes,\n",
    "                                        box_scale_factors, regress,\n",
    "                                        overlap_threshold)\n",
    "    encoded_boxes = encoded_boxes.reshape(-1, len(prior_boxes), 5)\n",
    "    best_iou = encoded_boxes[:, :, -1].max(axis=0)\n",
    "    best_iou_indices = encoded_boxes[:, :, -1].argmax(axis=0)\n",
    "    best_iou_mask = best_iou > 0\n",
    "    best_iou_indices = best_iou_indices[best_iou_mask]\n",
    "    num_assigned_boxes = len(best_iou_indices)\n",
    "    encoded_boxes = encoded_boxes[:, best_iou_mask, :]\n",
    "    assignments[best_iou_mask, :4] = encoded_boxes[best_iou_indices,\n",
    "                                            np.arange(num_assigned_boxes),\n",
    "                                            :4]\n",
    "\n",
    "    assignments[:, 4][best_iou_mask] = 0\n",
    "    #assignments[:, 5:-8][best_iou_mask] = ground_truth_data[best_iou_indices, 5:]\n",
    "    assignments[:, 5:][best_iou_mask] = ground_truth_data[best_iou_indices, 5:]\n",
    "    #assignments[:, -8][best_iou_mask] = 1\n",
    "    return assignments\n",
    "\n",
    "def load_model_configurations(model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        model: A SSD model with PriorBox layers that indicate the\n",
    "        parameters of the prior boxes to be created.\n",
    "\n",
    "    Returns:\n",
    "        model_configurations: A dictionary of the model parameters.\n",
    "    \"\"\"\n",
    "    model_configurations = []\n",
    "    for layer in model.layers:\n",
    "        layer_type = layer.__class__.__name__\n",
    "        if layer_type == 'PriorBox':\n",
    "            layer_data = {}\n",
    "            layer_data['layer_width'] = layer.input_shape[1]\n",
    "            layer_data['layer_height'] = layer.input_shape[2]\n",
    "            layer_data['min_size'] = layer.min_size\n",
    "            layer_data['max_size'] = layer.max_size\n",
    "            layer_data['aspect_ratios'] = layer.aspect_ratios\n",
    "            layer_data['num_prior'] = len(layer.aspect_ratios)\n",
    "            model_configurations.append(layer_data)\n",
    "    return model_configurations\n",
    "\n",
    "def create_prior_boxes(model):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        image_shape: The image shape (width, height) to the\n",
    "        input model.\n",
    "        model_configurations: The model configurations created by\n",
    "        load_model_configurations that indicate the parameters\n",
    "        inside the PriorBox layers.\n",
    "\n",
    "    Returns:\n",
    "        prior_boxes: A numpy array containing all prior boxes\n",
    "    \"\"\"\n",
    "    image_width, image_height = model.input_shape[1:3]\n",
    "    model_configurations = load_model_configurations(model)\n",
    "\n",
    "    boxes_parameters = []\n",
    "    for layer_config in model_configurations:\n",
    "        layer_width = layer_config[\"layer_width\"]\n",
    "        layer_height = layer_config[\"layer_height\"]\n",
    "        # RENAME: to num_aspect_ratios\n",
    "        num_priors = layer_config[\"num_prior\"]\n",
    "        aspect_ratios = layer_config[\"aspect_ratios\"]\n",
    "        min_size = layer_config[\"min_size\"]\n",
    "        max_size = layer_config[\"max_size\"]\n",
    "\n",
    "        # .5 is to locate every step in the center of the bounding box\n",
    "        step_x = 0.5 * (float(image_width) / float(layer_width))\n",
    "        step_y = 0.5 * (float(image_height) / float(layer_height))\n",
    "\n",
    "        linspace_x = np.linspace(step_x, image_width - step_x, layer_width)\n",
    "        linspace_y = np.linspace(step_y, image_height - step_y, layer_height)\n",
    "\n",
    "        centers_x, centers_y = np.meshgrid(linspace_x, linspace_y)\n",
    "        centers_x = centers_x.reshape(-1, 1)\n",
    "        centers_y = centers_y.reshape(-1, 1)\n",
    "\n",
    "        assert(num_priors == len(aspect_ratios))\n",
    "        prior_boxes = np.concatenate((centers_x, centers_y), axis=1)\n",
    "        prior_boxes = np.tile(prior_boxes, (1, 2 * num_priors))\n",
    "\n",
    "        box_widths = []\n",
    "        box_heights = []\n",
    "        for aspect_ratio in aspect_ratios:\n",
    "            if aspect_ratio == 1 and len(box_widths) == 0:\n",
    "                box_widths.append(min_size)\n",
    "                box_heights.append(min_size)\n",
    "            elif aspect_ratio == 1 and len(box_widths) > 0:\n",
    "                box_widths.append(np.sqrt(min_size * max_size))\n",
    "                box_heights.append(np.sqrt(min_size * max_size))\n",
    "            elif aspect_ratio != 1:\n",
    "                box_widths.append(min_size * np.sqrt(aspect_ratio))\n",
    "                box_heights.append(min_size / np.sqrt(aspect_ratio))\n",
    "        # we take half of the widths and heights since we are at the center\n",
    "        box_widths = 0.5 * np.array(box_widths)\n",
    "        box_heights = 0.5 * np.array(box_heights)\n",
    "\n",
    "        # Normalize to 0-1\n",
    "        prior_boxes[:, ::4] -= box_widths\n",
    "        prior_boxes[:, 1::4] -= box_heights\n",
    "        prior_boxes[:, 2::4] += box_widths\n",
    "        prior_boxes[:, 3::4] += box_heights\n",
    "        prior_boxes[:, ::2] /= image_width\n",
    "        prior_boxes[:, 1::2] /= image_height\n",
    "        prior_boxes = prior_boxes.reshape(-1, 4)\n",
    "\n",
    "        # clip to 0-1\n",
    "        layer_prior_boxes = np.minimum(np.maximum(prior_boxes, 0.0), 1.0)\n",
    "        boxes_parameters.append(layer_prior_boxes)\n",
    "\n",
    "    return np.concatenate(boxes_parameters, axis=0)\n",
    "\n",
    "def denormalize_box(box_data, original_image_shape):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        box_data: numpy array with shape (num_samples, 4) or\n",
    "        (num_samples, 4 + num_classes)\n",
    "        original_image_shape: The original image_shape (width, height)\n",
    "        of the loaded image shape.\n",
    "\n",
    "    Returns:\n",
    "        denormalize_box_data: A numpy array of shape (num_samples, 4)\n",
    "        or (num_samples, 4 + num_classes) containing the original box\n",
    "        coordinates.\n",
    "    \"\"\"\n",
    "    x_min = box_data[:, 0]\n",
    "    y_min = box_data[:, 1]\n",
    "    x_max = box_data[:, 2]\n",
    "    y_max = box_data[:, 3]\n",
    "    original_image_height, original_image_width = original_image_shape\n",
    "    x_min = x_min * original_image_width\n",
    "    y_min = y_min * original_image_height\n",
    "    x_max = x_max * original_image_width\n",
    "    y_max = y_max * original_image_height\n",
    "    denormalized_box_data = np.concatenate([x_min[:, None], y_min[:, None],\n",
    "                                    x_max[:, None], y_max[:, None]], axis=1)\n",
    "\n",
    "    if box_data.shape[1] > 4:\n",
    "        denormalized_box_data = np.concatenate([denormalized_box_data,\n",
    "                                            box_data[:, 4:]], axis=-1)\n",
    "    return denormalized_box_data\n",
    "\n",
    "def apply_non_max_suppression(boxes, iou_threshold=.2):\n",
    "    if len(boxes) == 0:\n",
    "            return []\n",
    "    selected_indices = []\n",
    "    x_min = boxes[:, 0]\n",
    "    y_min = boxes[:, 1]\n",
    "    x_max = boxes[:, 2]\n",
    "    y_max = boxes[:, 3]\n",
    "    classes = boxes[:, 4:]\n",
    "    sorted_box_indices = np.argsort(y_max)\n",
    "    while len(sorted_box_indices) > 0:\n",
    "            last = len(sorted_box_indices) - 1\n",
    "            i = sorted_box_indices[last]\n",
    "            selected_indices.append(i)\n",
    "            box = [x_min[i], y_min[i], x_max[i], y_max[i]]\n",
    "            box = np.asarray(box)\n",
    "            test_boxes = [x_min[sorted_box_indices[:last], None],\n",
    "                     y_min[sorted_box_indices[:last], None],\n",
    "                     x_max[sorted_box_indices[:last], None],\n",
    "                     y_max[sorted_box_indices[:last], None]]\n",
    "            test_boxes = np.concatenate(test_boxes, axis=-1)\n",
    "            iou = calculate_intersection_over_union(box, test_boxes)\n",
    "            current_class = np.argmax(classes[i])\n",
    "            box_classes = np.argmax(classes[sorted_box_indices[:last]], axis=-1)\n",
    "            class_mask = current_class == box_classes\n",
    "            overlap_mask = iou > iou_threshold\n",
    "            delete_mask = np.logical_and(overlap_mask, class_mask)\n",
    "            sorted_box_indices = np.delete(sorted_box_indices, np.concatenate(([last],\n",
    "                    np.where(delete_mask)[0])))\n",
    "    return boxes[selected_indices]\n",
    "\n",
    "def filter_boxes(predictions, num_classes, background_index=0,\n",
    "                                    lower_probability_threshold=.4):\n",
    "    predictions = np.squeeze(predictions)\n",
    "    box_classes = predictions[:, 4:(4 + num_classes)]\n",
    "    best_classes = np.argmax(box_classes, axis=-1)\n",
    "    best_probabilities = np.max(box_classes, axis=-1)\n",
    "    background_mask = best_classes != background_index\n",
    "    lower_bound_mask = lower_probability_threshold < best_probabilities\n",
    "    mask = np.logical_and(background_mask, lower_bound_mask)\n",
    "    selected_boxes = predictions[mask, :(4 + num_classes)]\n",
    "    return selected_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "#from .utils import load_image\n",
    "#from .utils import preprocess_images\n",
    "#from .preprocessing import load_image\n",
    "#from .preprocessing import preprocess_images\n",
    "#from .boxes import assign_prior_boxes\n",
    "\n",
    "class ImageGenerator(object):\n",
    "    \"\"\" Image generator with saturation, brightness, lighting, contrast,\n",
    "    horizontal flip and vertical flip transformations. It supports\n",
    "    bounding boxes coordinates.\n",
    "\n",
    "    TODO:\n",
    "        - Finish preprocess_images method.\n",
    "        - Add random crop method.\n",
    "        - Finish support for not using bounding_boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    #change ground_truth_data to ground_truth_data\n",
    "    def __init__(self, ground_truth_data, prior_boxes, num_classes,\n",
    "                box_scale_factors,\n",
    "                batch_size, image_size,\n",
    "                train_keys, validation_keys, path_prefix=None,\n",
    "                saturation_var=0.5,\n",
    "                brightness_var=0.5,\n",
    "                contrast_var=0.5,\n",
    "                lighting_std=0.5,\n",
    "                horizontal_flip_probability=0.5,\n",
    "                vertical_flip_probability=0.5,\n",
    "                do_crop=True,\n",
    "                crop_area_range=[0.75, 1.0],\n",
    "                aspect_ratio_range=[3./4., 4./3.]):\n",
    "\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        self.prior_boxes = prior_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.box_scale_factors = box_scale_factors\n",
    "        self.batch_size = batch_size\n",
    "        self.path_prefix = path_prefix\n",
    "        self.train_keys = train_keys\n",
    "        self.validation_keys = validation_keys\n",
    "        self.image_size = image_size\n",
    "        self.color_jitter = []\n",
    "        if saturation_var:\n",
    "            self.saturation_var = saturation_var\n",
    "            self.color_jitter.append(self.saturation)\n",
    "        if brightness_var:\n",
    "            self.brightness_var = brightness_var\n",
    "            self.color_jitter.append(self.brightness)\n",
    "        if contrast_var:\n",
    "            self.contrast_var = contrast_var\n",
    "            self.color_jitter.append(self.contrast)\n",
    "        self.lighting_std = lighting_std\n",
    "        self.horizontal_flip_probability = horizontal_flip_probability\n",
    "        self.vertical_flip_probability = vertical_flip_probability\n",
    "        self.do_crop = do_crop\n",
    "        self.crop_area_range = crop_area_range\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "\n",
    "    def _gray_scale(self, image_array):\n",
    "        return image_array.dot([0.299, 0.587, 0.114])\n",
    "\n",
    "    def saturation(self, image_array):\n",
    "        gray_scale = self._gray_scale(image_array)\n",
    "        alpha = 2.0 * np.random.random() * self.brightness_var\n",
    "        alpha = alpha + 1 - self.saturation_var\n",
    "        image_array = alpha * image_array + (1 - alpha) * gray_scale[:, :, None]\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def brightness(self, image_array):\n",
    "        alpha = 2 * np.random.random() * self.brightness_var\n",
    "        alpha = alpha + 1 - self.saturation_var\n",
    "        image_array = alpha * image_array\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def contrast(self, image_array):\n",
    "        gray_scale = (self._gray_scale(image_array).mean() *\n",
    "                        np.ones_like(image_array))\n",
    "        alpha = 2 * np.random.random() * self.contrast_var\n",
    "        alpha = alpha + 1 - self.contrast_var\n",
    "        image_array = image_array * alpha + (1 - alpha) * gray_scale\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def lighting(self, image_array):\n",
    "        covariance_matrix = np.cov(image_array.reshape(-1,3) /\n",
    "                                    255.0, rowvar=False)\n",
    "        eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)\n",
    "        noise = np.random.randn(3) * self.lighting_std\n",
    "        noise = eigen_vectors.dot(eigen_values * noise) * 255\n",
    "        image_array = image_array + noise\n",
    "        return np.clip(image_array, 0 , 255)\n",
    "\n",
    "    def horizontal_flip(self, image_array, box_corners):\n",
    "        if np.random.random() < self.horizontal_flip_probability:\n",
    "            image_array = image_array[:, ::-1]\n",
    "            box_corners[:, [0, 2]] = 1 - box_corners[:, [2, 0]]\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def vertical_flip(self, image_array, box_corners):\n",
    "        if (np.random.random() < self.vertical_flip_probability):\n",
    "            image_array = image_array[::-1]\n",
    "            box_corners[:, [1, 3]] = 1 - box_corners[:, [3, 1]]\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def transform(self, image_array, box_corners):\n",
    "        shuffle(self.color_jitter)\n",
    "        for jitter in self.color_jitter:\n",
    "            image_array = jitter(image_array)\n",
    "\n",
    "        if self.lighting_std:\n",
    "            image_array = self.lighting(image_array)\n",
    "\n",
    "        if self.horizontal_flip_probability > 0:\n",
    "            image_array, box_corners = self.horizontal_flip(image_array,\n",
    "                                                            box_corners)\n",
    "\n",
    "        if self.vertical_flip_probability > 0:\n",
    "            image_array, box_corners = self.vertical_flip(image_array,\n",
    "                                                            box_corners)\n",
    "\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def flow(self, mode='train'):\n",
    "            while True:\n",
    "                if mode =='train':\n",
    "                   # print('Train mode set')\n",
    "                    shuffle(self.train_keys)\n",
    "                    keys = self.train_keys\n",
    "                elif mode == 'val' or  mode == 'demo':\n",
    "                    #print('val/demo mode set')\n",
    "                    shuffle(self.validation_keys)\n",
    "                    keys = self.validation_keys\n",
    "                else:\n",
    "                    raise Exception('invalid mode: %s' % mode)\n",
    "\n",
    "                inputs = []\n",
    "                targets = []\n",
    "                for key in keys:\n",
    "                    \n",
    "                    image_path = self.path_prefix + key\n",
    "                    print('image_path', image_path)\n",
    "                    image_array = load_image(image_path, self.image_size)\n",
    "                    #print('image_array_before', image_array)\n",
    "                    box_corners = self.ground_truth_data[key].copy()\n",
    "                    \n",
    "                    if mode == 'train' or mode == 'demo':\n",
    "                        image_array, box_corners = self.transform(image_array,\n",
    "                                                                box_corners)\n",
    "                    #print('box_corners',box_corners)\n",
    "                    box_corners = assign_prior_boxes(self.prior_boxes,\n",
    "                                                    box_corners,\n",
    "                                                    self.num_classes,\n",
    "                                                    self.box_scale_factors)\n",
    "                    #print('box_corners_with anchors',box_corners)\n",
    "                    inputs.append(image_array)\n",
    "                    #print('inputs', inputs)\n",
    "                    targets.append(box_corners)\n",
    "                    #print('targets', targets)\n",
    "                    if len(targets) == self.batch_size:\n",
    "                        inputs = np.asarray(inputs)\n",
    "                        targets = np.asarray(targets)\n",
    "                        if mode == 'train' or mode == 'val':\n",
    "                            inputs = preprocess_images(inputs)\n",
    "                            yield self._wrap_in_dictionary(inputs, targets)\n",
    "                        if mode == 'demo':\n",
    "                            yield self._wrap_in_dictionary(inputs, targets)\n",
    "                        inputs = []\n",
    "                        targets = []\n",
    "\n",
    "    def _wrap_in_dictionary(self, image_array, targets):\n",
    "        return [{'input_1':image_array},\n",
    "                {'predictions':targets}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[ 0.          0.          0.06315789  0.06315789]\n",
      " [ 0.          0.          0.08386857  0.04851323]\n",
      " [ 0.          0.          0.04851323  0.08386857]\n",
      " ..., \n",
      " [ 0.17473088  0.          0.82526912  1.        ]\n",
      " [ 0.          0.23441888  1.          0.76558112]\n",
      " [ 0.23441888  0.          0.76558112  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# setting parameters for data augmentation generator\n",
    "prior_boxes = create_prior_boxes(model)\n",
    "print(batch_size)\n",
    "print (prior_boxes)\n",
    "image_generator = ImageGenerator(ground_truth_data,\n",
    "                                 prior_boxes,\n",
    "                                 num_classes,\n",
    "                                 box_scale_factors,\n",
    "                                 batch_size,\n",
    "                                 image_shape[0:2],\n",
    "                                 train_keys, validation_keys,\n",
    "                                 image_prefix,\n",
    "                                 vertical_flip_probability=0.5,\n",
    "                                 horizontal_flip_probability=0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating callbacks\n",
    "learning_rate_schedule = LearningRateScheduler(scheduler)\n",
    "model_names = (trained_models_filename)\n",
    "model_checkpoint = ModelCheckpoint(model_names,\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=False)\n",
    "\n",
    "model_tensorboard = TensorBoard(log_dir='./logs/200-epoch', \n",
    "                                         histogram_freq=10,\n",
    "                                         #batch_size = 8,\n",
    "                                         write_graph=True,\n",
    "                                         write_images=True,\n",
    "                                          \n",
    "                                         #write_grads=True,\n",
    "                                         embeddings_freq =5,\n",
    "                                         embeddings_layer_names = 'conv3_3, conv4_3' \n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007826.jpg\n",
      "Epoch 1/15\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004676.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002912.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006722.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007263.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007614.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002291.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005854.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003690.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007365.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004273.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006597.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000470.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002404.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003088.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001749.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002061.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000958.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007751.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007762.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006348.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005231.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006748.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000469.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004286.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003491.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002924.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003256.jpg\n",
      "  1/501 [..............................] - ETA: 295s - loss: 4.8535 - acc: 0.3189image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005450.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002452.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007680.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001691.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004204.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000169.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003042.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004595.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000104.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004424.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005640.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003011.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004247.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002261.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001152.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003398.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007243.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003642.jpg\n",
      "  2/501 [..............................] - ETA: 255s - loss: 4.7996 - acc: 0.3019image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000677.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003508.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006100.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007535.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006940.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002140.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002184.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003365.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002208.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001237.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004341.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004890.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003983.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005989.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000523.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006466.jpg\n",
      "  3/501 [..............................] - ETA: 243s - loss: 4.8174 - acc: 0.2937image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003511.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004912.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001142.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003575.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004991.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007810.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005729.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001854.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000806.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004500.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003194.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007417.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004647.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001771.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007138.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001633.jpg\n",
      "  4/501 [..............................] - ETA: 236s - loss: 4.9319 - acc: 0.3208image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002220.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000189.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004654.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002366.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006236.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006088.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000780.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007672.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002810.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006585.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007619.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001555.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003946.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003105.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000496.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006214.jpg\n",
      "  5/501 [..............................] - ETA: 233s - loss: 4.8932 - acc: 0.3103image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007667.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002759.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006163.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003244.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004856.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002401.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003344.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004421.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003133.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002260.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004585.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005079.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003696.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007565.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004808.jpg\n",
      "  6/501 [..............................] - ETA: 231s - loss: 4.9596 - acc: 0.3093image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002727.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000270.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001726.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007197.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000311.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004691.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007940.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007296.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004719.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000109.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001451.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007071.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003834.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006062.jpg\n",
      "  7/501 [..............................] - ETA: 230s - loss: 4.9319 - acc: 0.3137image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007932.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001759.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007570.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004013.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006773.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005222.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004902.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007230.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003961.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003134.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006209.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002709.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003603.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002625.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000589.jpg\n",
      "  8/501 [..............................] - ETA: 229s - loss: 5.0014 - acc: 0.3121image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001408.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000895.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004785.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007020.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005875.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003990.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004878.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005812.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006319.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003202.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003620.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001772.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000520.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000605.jpg\n",
      "  9/501 [..............................] - ETA: 228s - loss: 5.0286 - acc: 0.3077image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005340.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001204.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004623.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003396.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000077.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006130.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002332.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003293.jpg\n",
      " 10/501 [..............................] - ETA: 227s - loss: 4.9942 - acc: 0.3126image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004142.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002001.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004783.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004846.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007715.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007572.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005293.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001421.jpg\n",
      " 11/501 [..............................] - ETA: 226s - loss: 5.0036 - acc: 0.3142image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003521.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003106.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003145.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002800.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001453.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005970.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005016.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005686.jpg\n",
      " 12/501 [..............................] - ETA: 224s - loss: 5.1388 - acc: 0.3153image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001315.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001161.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000888.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003868.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002460.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005262.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003413.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005990.jpg\n",
      " 13/501 [..............................] - ETA: 223s - loss: 5.1098 - acc: 0.3159image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007629.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006678.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005114.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005761.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004046.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001209.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004060.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007077.jpg\n",
      " 14/501 [..............................] - ETA: 222s - loss: 5.0986 - acc: 0.3209image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003994.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002385.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000711.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006071.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002238.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004943.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005270.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004392.jpg\n",
      " 15/501 [..............................] - ETA: 220s - loss: 5.0577 - acc: 0.3203image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003890.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000695.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005682.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003038.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005813.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003127.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003948.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005850.jpg\n",
      " 16/501 [..............................] - ETA: 219s - loss: 5.0323 - acc: 0.3197image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004106.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005455.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007683.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003154.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003594.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001292.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000222.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001316.jpg\n",
      " 17/501 [>.............................] - ETA: 219s - loss: 5.0743 - acc: 0.3195image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001541.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006123.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000142.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002690.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000036.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005129.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000601.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003924.jpg\n",
      " 18/501 [>.............................] - ETA: 218s - loss: 5.0313 - acc: 0.3145image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007691.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005752.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005064.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007343.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002278.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006035.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001898.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006799.jpg\n",
      " 19/501 [>.............................] - ETA: 217s - loss: 5.0263 - acc: 0.3131image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004066.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007204.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006739.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000949.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004149.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000684.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000525.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000209.jpg\n",
      " 20/501 [>.............................] - ETA: 217s - loss: 5.0009 - acc: 0.3114image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007809.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001523.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003242.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005003.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006230.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005230.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003057.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006323.jpg\n",
      " 21/501 [>.............................] - ETA: 216s - loss: 5.0375 - acc: 0.3107image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007856.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005630.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005909.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001526.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001352.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001522.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003808.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005124.jpg\n",
      " 22/501 [>.............................] - ETA: 216s - loss: 5.0139 - acc: 0.3082image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007200.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004761.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003231.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005453.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003354.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003373.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001970.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003484.jpg\n",
      " 23/501 [>.............................] - ETA: 215s - loss: 4.9967 - acc: 0.3062image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004073.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002063.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005613.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007305.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001903.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006433.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007168.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006848.jpg\n",
      " 24/501 [>.............................] - ETA: 214s - loss: 4.9785 - acc: 0.3060image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006105.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005062.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003290.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002767.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004186.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003259.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002307.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002962.jpg\n",
      " 25/501 [>.............................] - ETA: 214s - loss: 4.9516 - acc: 0.3039image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005191.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002910.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001236.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003085.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006392.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000498.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006427.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002293.jpg\n",
      " 26/501 [>.............................] - ETA: 213s - loss: 4.9732 - acc: 0.3025image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001192.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004339.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007292.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001418.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005527.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007662.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000948.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001499.jpg\n",
      " 27/501 [>.............................] - ETA: 212s - loss: 4.9526 - acc: 0.3030image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004686.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000854.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003856.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004459.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002880.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001821.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005090.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006417.jpg\n",
      " 28/501 [>.............................] - ETA: 212s - loss: 4.9547 - acc: 0.3044image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001725.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/003565.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002117.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005248.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007606.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/005888.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007007.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/004555.jpg\n",
      " 29/501 [>.............................] - ETA: 212s - loss: 4.9538 - acc: 0.3073image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000064.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002270.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/001734.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006247.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/002367.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/000748.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/007148.jpg\n",
      "image_path ../datasets/VOCdevkit/VOC2007/JPEGImages/006696.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7e38ab536148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                     validation_steps=int(len(validation_keys) / batch_size))\n\u001b[0m",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1888\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1889\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1890\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2227\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2229\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/a/anaconda3/envs/keras/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training model with real-time data augmentation\n",
    "model.fit_generator(image_generator.flow(mode='train'),\n",
    "                    steps_per_epoch=int(len(train_keys) / batch_size),\n",
    "                    epochs = num_epochs, verbose = 1,\n",
    "                    callbacks=[model_checkpoint, learning_rate_schedule],\n",
    "                    validation_data=image_generator.flow(mode='val'),\n",
    "                    validation_steps=int(len(validation_keys) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from utils.datasets import DataManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_name = 'VOC2007'\n",
    "dataset_manager = DataManager(dataset_name)\n",
    "ground_truth_data = dataset_manager.load_data()\n",
    "class_names = dataset_manager.class_names\n",
    "print('Found:',len(ground_truth_data),'images \\nClass names: \\n', class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset_name = 'VOC2007'\n",
    "class_names = ['background','diningtable', 'chair']\n",
    "dataset_manager = DataManager(dataset_name, class_names)\n",
    "ground_truth_data = dataset_manager.load_data()\n",
    " \n",
    "print('Found:',len(ground_truth_data),'images \\nClass names: \\n', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior/default box creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from models.ssd import SSD300\n",
    "#from utils.boxes import create_prior_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SSD300()\n",
    "prior_boxes = create_prior_boxes(model)\n",
    "print('Prior boxes shape:', prior_boxes.shape)\n",
    "print('Prior box example:', prior_boxes[777])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from utils.visualizer import draw_image_boxes\n",
    "#from utils.preprocessing import load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import cv2\n",
    "except ImportError:\n",
    "    cv2 = None\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "import numpy as np\n",
    "from utils.boxes import denormalize_box\n",
    "\n",
    "# with openCV\n",
    "def draw_video_boxes(box_data, original_image_array, arg_to_class, colors, font):\n",
    "    if len(box_data) == 0:\n",
    "        return\n",
    "    x_min = box_data[:, 0]\n",
    "    y_min = box_data[:, 1]\n",
    "    x_max = box_data[:, 2]\n",
    "    y_max = box_data[:, 3]\n",
    "    classes = box_data[:, 4:]\n",
    "    num_boxes = len(box_data)\n",
    "    for box_arg in range(num_boxes):\n",
    "        x_min_box = int(x_min[box_arg])\n",
    "        y_min_box = int(y_min[box_arg])\n",
    "        x_max_box = int(x_max[box_arg])\n",
    "        y_max_box = int(y_max[box_arg])\n",
    "        box_class_scores = classes[box_arg]\n",
    "        label_arg = np.argmax(box_class_scores)\n",
    "        score = box_class_scores[label_arg]\n",
    "        class_name = arg_to_class[label_arg]\n",
    "        color = colors[label_arg]\n",
    "        display_text = '{:0.2f}, {}'.format(score, class_name)\n",
    "        cv2.rectangle(original_image_array, (x_min_box, y_min_box),\n",
    "                                (x_max_box, y_max_box), color, 2)\n",
    "        cv2.putText(original_image_array, display_text,\n",
    "                    (x_min_box, y_min_box - 30), font,\n",
    "                    .7, color, 1, cv2.LINE_AA)\n",
    "\n",
    "# with matplotlib\n",
    "def draw_image_boxes(box_data, original_image_array,\n",
    "                    arg_to_class=None, colors=None,\n",
    "                                    normalized=True):\n",
    "    if len(box_data) == 0:\n",
    "        return None\n",
    "    if normalized:\n",
    "        box_data = denormalize_box(box_data, original_image_array.shape[0:2][::-1])\n",
    "    original_image_array = original_image_array.astype('uint8')\n",
    "    figure, axis = plt.subplots(1)\n",
    "    axis.imshow(original_image_array)\n",
    "    x_min = box_data[:, 0]\n",
    "    y_min = box_data[:, 1]\n",
    "    x_max = box_data[:, 2]\n",
    "    y_max = box_data[:, 3]\n",
    "    if box_data.shape[1] > 4:\n",
    "        classes = box_data[:, 4:]\n",
    "        with_classes = True\n",
    "        if colors is None:\n",
    "            num_classes = classes.shape[1]\n",
    "            colors = get_colors(num_classes)\n",
    "    else:\n",
    "        with_classes = False\n",
    "    num_boxes = len(box_data)\n",
    "    for box_arg in range(num_boxes):\n",
    "        x_min_box = int(x_min[box_arg])\n",
    "        y_min_box = int(y_min[box_arg])\n",
    "        x_max_box = int(x_max[box_arg])\n",
    "        y_max_box = int(y_max[box_arg])\n",
    "        box_width = x_max_box - x_min_box\n",
    "        box_height = y_max_box - y_min_box\n",
    "        if with_classes:\n",
    "            box_class_scores = classes[box_arg]\n",
    "            label_arg = np.argmax(box_class_scores)\n",
    "            score = box_class_scores[label_arg]\n",
    "            class_name = arg_to_class[label_arg]\n",
    "            color = colors[label_arg]\n",
    "            display_text = '{:0.2f}, {}'.format(score, class_name)\n",
    "            x_text = x_min_box\n",
    "            y_text = y_min_box\n",
    "            axis.text(x_text, y_text, display_text,\n",
    "                    bbox={'facecolor':color, 'alpha':0.5, 'pad':10})\n",
    "        else:\n",
    "            color = 'r'\n",
    "\n",
    "        rectangle = plt.Rectangle((x_min_box, y_min_box),\n",
    "                                    box_width, box_height,\n",
    "                                    linewidth=1, edgecolor=color,\n",
    "                                    facecolor='none')\n",
    "        axis.add_patch(rectangle)\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(image_1, image_2, title_1='original image',\n",
    "                            title_2='transformed image'):\n",
    "    plt.figure(1)\n",
    "    plt.subplot(121)\n",
    "    plt.title(title_1)\n",
    "    plt.imshow(image_1.astype('uint8'))\n",
    "    plt.subplot(122)\n",
    "    plt.title(title_2)\n",
    "    plt.imshow(image_2.astype('uint8'))\n",
    "    plt.show()\n",
    "\n",
    "#def get_colors(num_colors):\n",
    "#    #this is a horrible function...\n",
    "#    # taken from: https://matplotlib.org/examples/color/named_colors.html\n",
    "#    colors = dict(mcolors.BASE_COLORS, **mcolors.CSS4_COLORS)\n",
    "#\n",
    "#    # Sort colors by hue, saturation, value and name.\n",
    "#    by_hsv = sorted((tuple(mcolors.rgb_to_hsv(mcolors.to_rgba(color)[:3])), name)\n",
    "#                    for name, color in colors.items())\n",
    "#    sorted_names = [name for hsv, name in by_hsv]\n",
    "#    color_args = (np.linspace(0, 1, num_colors) * (len(sorted_names)-1)).astype(int)\n",
    "#    sorted_names = np.asarray(sorted_names)\n",
    "#    selected_color_names = sorted_names[color_args].tolist()\n",
    "#    selected_color_values = [colors[color_name]\n",
    "#                            for color_name in selected_color_names]\n",
    "#\n",
    "#    return selected_color_values\n",
    "#\n",
    "def get_colors(num_colors=21):\n",
    "    return plt.cm.hsv(np.linspace(0, 1, num_colors)).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = '../images/fish-bike.jpg'\n",
    "input_shape = model.input_shape[1:3]\n",
    "image_array = load_image(image_path, input_shape)\n",
    "box_coordinates = prior_boxes[6080:6084, :]\n",
    "draw_image_boxes(box_coordinates, image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#box_coordinates = prior_boxes[5400:6900, :]\n",
    "#draw_image_boxes(box_coordinates, image_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_name = sorted(list(ground_truth_data.keys()))[110]\n",
    "box_data = ground_truth_data[image_name]\n",
    "print('Data sample: \\n', box_data)\n",
    "image_prefix = dataset_manager.image_prefix\n",
    "image_path = image_prefix + image_name\n",
    "class_decoder = dataset_manager.arg_to_class\n",
    "image_array = load_image(image_path, input_shape)\n",
    "draw_image_boxes(box_data, image_array, class_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigned prior boxes without regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from utils.boxes import assign_prior_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#prior_box_manager = PriorBoxManager(prior_boxes, num_classes=num_classes)\n",
    "num_classes = len(class_names)\n",
    "box_scale_factors = [.1, .1, .2, .2]\n",
    "assigned_boxes = assign_prior_boxes(prior_boxes, box_data,\n",
    "                                    num_classes, box_scale_factors,\n",
    "                                    regress=False,\n",
    "                                    overlap_threshold=.6)\n",
    "positive_mask = assigned_boxes[:, 4] != 1\n",
    "positive_boxes = assigned_boxes[positive_mask]\n",
    "draw_image_boxes(positive_boxes, image_array, class_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigned prior boxes with regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "assigned_regressed_boxes = assign_prior_boxes(prior_boxes, box_data,\n",
    "                                              num_classes, box_scale_factors,\n",
    "                                              regress=True)\n",
    "positive_mask = assigned_regressed_boxes[:, 4] != 1\n",
    "encoded_positive_boxes = assigned_regressed_boxes[positive_mask]\n",
    "draw_image_boxes(assigned_regressed_boxes, image_array, class_decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigned decoded boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from utils.boxes import decode_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "assigned_decoded_boxes = decode_boxes(assigned_regressed_boxes,\n",
    "                                      prior_boxes, box_scale_factors)\n",
    "decoded_positive_boxes = assigned_decoded_boxes[positive_mask]\n",
    "draw_image_boxes(decoded_positive_boxes, image_array, class_decoder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from utils.data_augmentation import ImageGenerator\n",
    "#from utils.preprocessing import load_image\n",
    "#from utils.visualizer import plot_images\n",
    "#from utils.train import split_data\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_keys, validation_keys = split_data(ground_truth_data, training_ratio=.8)\n",
    "image_generator = ImageGenerator(ground_truth_data,\n",
    "                                 prior_boxes,\n",
    "                                 num_classes,\n",
    "                                 box_scale_factors,\n",
    "                                 batch_size,\n",
    "                                 input_shape,\n",
    "                                 train_keys, validation_keys,\n",
    "                                 image_prefix,\n",
    "                                 vertical_flip_probability=0,\n",
    "                                 horizontal_flip_probability=2)\n",
    "\n",
    "generated_data = next(image_generator.flow(mode='demo'))\n",
    "generated_input = generated_data[0]['input_1']\n",
    "#print('generated_data', generated_data)\n",
    "generated_output = generated_data[1]['predictions']\n",
    "generated_image = np.squeeze(generated_input[0]).astype('uint8')\n",
    "validation_image_name = image_prefix + validation_keys[0]\n",
    "original_image = load_image(validation_image_name, input_shape)\n",
    "plot_images(original_image, generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modified ground-truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generated_encoded_boxes = np.squeeze(generated_output)\n",
    "generated_boxes = decode_boxes(generated_encoded_boxes, prior_boxes,\n",
    "                                                  box_scale_factors)\n",
    "positive_mask = generated_boxes[:, 4] != 1\n",
    "generated_positive_boxes = generated_boxes[positive_mask]\n",
    "draw_image_boxes(generated_positive_boxes, generated_image, class_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "#from utils.inference import predict\n",
    "#from utils.preprocessing import get_image_size\n",
    "#from utils.preprocessing import load_pil_image\n",
    "from scipy.misc import imread\n",
    "#from utils.preprocessing import image_to_array\n",
    "from utils.datasets import get_class_names\n",
    "from utils.datasets import get_arg_to_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#from .preprocessing import preprocess_images\n",
    "#from .boxes import decode_boxes\n",
    "#from .boxes import filter_boxes\n",
    "#from .preprocessing import resize_image_array\n",
    "#from .boxes import denormalize_box\n",
    "#from .boxes import apply_non_max_suppression\n",
    "#from .tf_boxes import apply_non_max_suppression\n",
    "\n",
    "session = tf.Session(config=tf.ConfigProto(device_count={'GPU':0}))\n",
    "\n",
    "def apply_non_max_suppression(box_data, iou_treshold=.5,\n",
    "                                        max_output_size=100):\n",
    "    num_boxes = len(box_data)\n",
    "    coordinates = tf.placeholder(dtype='float32', shape=(num_boxes, 4))\n",
    "    scores = tf.placeholder(dtype='float32', shape=(num_boxes))\n",
    "    class_prob = np.max(box_data[:, 4:], axis=-1)\n",
    "    feed_dict = {coordinates: box_data[:, 0:4], scores: class_prob}\n",
    "    non_maximum_supression = tf.image.non_max_suppression(coordinates,\n",
    "                                                    scores, max_output_size,\n",
    "                                                              iou_treshold)\n",
    "    indices = session.run(non_maximum_supression, feed_dict=feed_dict)\n",
    "    return box_data[indices]\n",
    "\n",
    "\n",
    "def predict(model, image_array, prior_boxes, original_image_shape,\n",
    "            num_classes=21, lower_probability_threshold=.1,\n",
    "            iou_threshold=.5, background_index=0,\n",
    "            box_scale_factors=[.1, .1, .2, .2]):\n",
    "\n",
    "    image_array = image_array.astype('float32')\n",
    "    input_size = model.input_shape[1:3]\n",
    "    image_array = resize_image_array(image_array, input_size)\n",
    "    image_array = np.expand_dims(image_array, axis=0)\n",
    "    image_array = preprocess_images(image_array)\n",
    "    predictions = model.predict(image_array)\n",
    "    predictions = np.squeeze(predictions)\n",
    "    decoded_predictions = decode_boxes(predictions, prior_boxes,\n",
    "                                              box_scale_factors)\n",
    "    selected_boxes = filter_boxes(decoded_predictions,\n",
    "                num_classes, background_index,\n",
    "                lower_probability_threshold)\n",
    "    if len(selected_boxes) == 0:\n",
    "        return None\n",
    "    selected_boxes = denormalize_box(selected_boxes, original_image_shape)\n",
    "    selected_boxes = apply_non_max_suppression(selected_boxes, iou_threshold)\n",
    "    return selected_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = model.input_shape[1:3]\n",
    "weights_path = '../trained_models/weights_SSD300.hdf5'\n",
    "image_paths = glob.glob('../images/*.jpg')\n",
    "class_names = get_class_names(dataset_name)\n",
    "class_decoder = get_arg_to_class(class_names)\n",
    "num_classes = len(class_names)\n",
    "probability_threshold = .5\n",
    "#model = SSD300(num_classes=21, weights_path='../trained_models/weights_SSD300.hdf5')\n",
    "model = SSD300(num_classes=21, weights_path= '../trained_models/model_checkpoints/ssd300_weights.106-2.969.hdf5')\n",
    "for image_path in image_paths:\n",
    "    image_array = load_image(image_path, input_shape)\n",
    "    original_image_array = image_to_array(load_pil_image(image_path)) \n",
    "    original_image_size = get_image_size(image_path)\n",
    "    selected_boxes = predict(model, image_array, prior_boxes, original_image_size,\n",
    "                             num_classes, probability_threshold, .5)\n",
    "    draw_image_boxes(selected_boxes, original_image_array, class_decoder,\n",
    "                                                         normalized=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 2\n",
    "## Training with Udacity db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    frame  xmin  ymin  xmax  ymax       label\n",
      "0  object-dataset/1478019952686311006.jpg   950   574  1004   620         car\n",
      "1  object-dataset/1478019952686311006.jpg  1748   482  1818   744  pedestrian\n",
      "2  object-dataset/1478019953180167674.jpg   872   586   926   632         car\n",
      "3  object-dataset/1478019953689774621.jpg   686   566   728   618       truck\n",
      "4  object-dataset/1478019953689774621.jpg   716   578   764   622         car\n",
      "   xmin  ymin  xmax  ymax                                             frame  \\\n",
      "0   785   533   905   644  object-detection-crowdai/1479498371963069978.jpg   \n",
      "1    89   551   291   680  object-detection-crowdai/1479498371963069978.jpg   \n",
      "2   268   546   383   650  object-detection-crowdai/1479498371963069978.jpg   \n",
      "3   455   522   548   615  object-detection-crowdai/1479498371963069978.jpg   \n",
      "4   548   522   625   605  object-detection-crowdai/1479498371963069978.jpg   \n",
      "\n",
      "   label  \n",
      "0    car  \n",
      "1    car  \n",
      "2    car  \n",
      "3  truck  \n",
      "4  truck  \n",
      "df1.shape (74157, 6)\n",
      "df2.shape (72064, 6)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "batch_size = 8\n",
    "num_epochs = 15\n",
    "num_classes = 4\n",
    "image_shape = (300, 300 ,3)\n",
    "optimizer = Adam(lr=3e-4)\n",
    "\n",
    "root_prefix = '/home/a/SDC/P5_Final_Submission/'\n",
    "path_prefix = root_prefix\n",
    "\n",
    "ground_data_prefix = root_prefix# + 'Annotations/'\n",
    "image_prefix = root_prefix# + 'JPEGImages/'\n",
    " \n",
    "dataset_name = 'Alex'\n",
    "input_shape = (300, 300, 3)\n",
    "weights_path = '../trained_models/weights_SSD300.hdf5'\n",
    "trained_models_path = '../trained_models/model_checkpoints/udacity/'\n",
    "trained_models_filename = (trained_models_path +\n",
    "                        'ssd300_weights.{epoch:03d}-{val_loss:.3f}.hdf5')\n",
    "\n",
    "voc_classes = ['background','car','pedestrian','truck']\n",
    "\n",
    "frozen_layers = ['input_1', 'conv1_1', 'conv1_2', 'pool1',\n",
    "                'conv2_1', 'conv2_2', 'pool2',\n",
    "                'conv3_1', 'conv3_2', 'conv3_3', 'pool3']\n",
    "#['background', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "#'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike', \n",
    "#'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "#voc_classes = ['Aeroplane', 'Bicycle', 'Bird', 'Boat', 'Bottle',\n",
    "               #'Bus', 'Car', 'Cat', 'Chair', 'Cow', 'Diningtable',\n",
    "               #'Dog', 'Horse','Motorbike', 'Person', 'Pottedplant',\n",
    "               #'Sheep', 'Sofa', 'Train', 'Tvmonitor']\n",
    "#num cclasses = hotcoded + background\n",
    "NUM_CLASSES = len(voc_classes)\n",
    "\n",
    "num_classes = NUM_CLASSES\n",
    "\n",
    "%matplotlib inline\n",
    "cols1 = [\"frame\", \"xmin\", \"ymin\", \"xmax\", \"ymax\", \"occluded\", \"label\", \"attributes\"]\n",
    "df1 = pd.read_csv('/home/a/SDC/P5_Final_Submission/object-dataset/labels.csv', names=cols1, delimiter=\",\")\n",
    "\n",
    "df1 = df1.query(\"label != 'trafficLight' and label != 'biker'\")\n",
    "\n",
    "# just keep the image and gt box coordinates\n",
    "df1.drop(['occluded', 'attributes'], axis=1, inplace=True)\n",
    "df1.frame = 'object-dataset/' + df1.frame\n",
    "print(df1.head())\n",
    "df1 = df1.reset_index(drop=True)\n",
    "\n",
    "cols2 = [\"xmin\", \"ymin\", \"xmax\", \"ymax\", \"frame\", \"label\"]\n",
    "df2 = pd.read_csv('/home/a/SDC/P5_Final_Submission/object-detection-crowdai/labels_crowdai.csv')\n",
    "\n",
    "df2.drop('Preview URL', axis=1, inplace=True)\n",
    "df2.columns = cols2\n",
    "df2['label'] = df2['label'].str.lower()\n",
    "df2.frame = 'object-detection-crowdai/' + df2.frame\n",
    "print(df2.head())\n",
    "\n",
    "print('df1.shape', df1.shape)\n",
    "print('df2.shape', df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>object-dataset/1478019952686311006.jpg</td>\n",
       "      <td>950</td>\n",
       "      <td>574</td>\n",
       "      <td>1004</td>\n",
       "      <td>620</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>object-dataset/1478019952686311006.jpg</td>\n",
       "      <td>1748</td>\n",
       "      <td>482</td>\n",
       "      <td>1818</td>\n",
       "      <td>744</td>\n",
       "      <td>pedestrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>object-dataset/1478019953180167674.jpg</td>\n",
       "      <td>872</td>\n",
       "      <td>586</td>\n",
       "      <td>926</td>\n",
       "      <td>632</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>object-dataset/1478019953689774621.jpg</td>\n",
       "      <td>686</td>\n",
       "      <td>566</td>\n",
       "      <td>728</td>\n",
       "      <td>618</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>object-dataset/1478019953689774621.jpg</td>\n",
       "      <td>716</td>\n",
       "      <td>578</td>\n",
       "      <td>764</td>\n",
       "      <td>622</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    frame  xmin  ymin  xmax  ymax       label\n",
       "0  object-dataset/1478019952686311006.jpg   950   574  1004   620         car\n",
       "1  object-dataset/1478019952686311006.jpg  1748   482  1818   744  pedestrian\n",
       "2  object-dataset/1478019953180167674.jpg   872   586   926   632         car\n",
       "3  object-dataset/1478019953689774621.jpg   686   566   728   618       truck\n",
       "4  object-dataset/1478019953689774621.jpg   716   578   764   622         car"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp1 = df1.groupby('label').size()\n",
    "grp1 / grp1.sum() * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grp2 = df2.groupby('label').size()\n",
    "grp2 / grp2.sum() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "df[df.ymax == 0] # this is error (ymax shouldn't be zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.ymax != 0]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encode classes into ones and zeros that are suatable for the SSD training\n",
    "classes = df.label.unique()\n",
    "  \n",
    "print('prior transform classes',classes)\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "\n",
    "\n",
    "le.transform(classes)\n",
    "print ('transformed classes',le.transform(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le.transform(['truck'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw(key):\n",
    "    image = mpimg.imread('/home/a/SDC/P5_Final_Submission/' + key)\n",
    "    data = df[df.frame == key]\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, len(classes))).tolist()\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    for idx, img in data.iterrows():\n",
    "        cv2.rectangle(image, (img.xmin, img.ymin), (img.xmax, img.ymax),(0, 255, 0), 6)\n",
    "        coords = (img.xmin, img.ymin), img.xmax-img.xmin+1, img.ymax-img.ymin+1\n",
    "        color = colors[le.transform([img.label])[0]]\n",
    "        ax.add_patch(plt.Rectangle(*coords, fill=False, edgecolor=color, linewidth=2))\n",
    "        ax.text(img.xmin, img.ymin, img.label, bbox={'facecolor':color, 'alpha':0.5})\n",
    "\n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw('object-detection-crowdai/1479498371963069978.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw('object-detection-crowdai/1479503036282378933.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = mpimg.imread('/home/a/SDC/P5_Final_Submission/object-detection-crowdai/1479498371963069978.jpg')\n",
    "h, w, _ = img.shape\n",
    "#coordinates of the object(s) in the image,eg: (x/img_width, y/img_height, (x+w)/img_width, (y+h)/img_height).\n",
    "df.xmin = df.xmin / w\n",
    "df.xmax = df.xmax / w\n",
    "\n",
    "df.ymin = df.ymin / h\n",
    "df.ymax = df.ymax / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(df.label)\n",
    "print(lb.classes_)\n",
    "\n",
    "alex=lb.transform(['car', 'pedestrian', 'truck', 'car', 'pedestrian'])\n",
    "print('alex',alex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "gt_sample = pickle.load(open('/home/a/WA/delete-me/single_shot_multibox_detector/pikls/gt_pascal.pkl', 'rb'))\n",
    "gt_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the picle hotcoded datasets with background\n",
    "dataset = dict()\n",
    "for idx, row in df.iterrows():\n",
    "    coords = [row.xmin, row.ymin, row.xmax, row.ymax]\n",
    "    label_encoded = lb.transform([row.label]).ravel()\n",
    "    label_encoded = np.insert(label_encoded,0, 0, axis=0)\n",
    "    #print('label_encoded',label_encoded)\n",
    "    val = np.array([np.hstack((coords, label_encoded))])\n",
    "    \n",
    "    current = dataset.get(row.frame)\n",
    "    if current is None:\n",
    "        dataset[row.frame] = val\n",
    "    else:\n",
    "        dataset[row.frame] = np.vstack((current, val))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the picle hotcoded datasets without background\n",
    "dataset = dict()\n",
    "for idx, row in df.iterrows():\n",
    "    coords = [row.xmin, row.ymin, row.xmax, row.ymax]\n",
    "    label_encoded = lb.transform([row.label]).ravel()\n",
    "    \n",
    "    val = np.array([np.hstack((coords, label_encoded))])\n",
    "    \n",
    "    current = dataset.get(row.frame)\n",
    "    if current is None:\n",
    "        dataset[row.frame] = val\n",
    "    else:\n",
    "        dataset[row.frame] = np.vstack((current, val))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gt = pickle.load(open('/home/a/WA/delete-me/single_shot_multibox_detector/pikls/ground-truth.pkl', 'rb'))\n",
    "\n",
    "\n",
    "#pickle.dump(dataset, open(\"/home/a/WA/delete-me/single_shot_multibox_detector/pikls/ground-truth.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "#from .utils import load_image\n",
    "#from .utils import preprocess_images\n",
    "#from .preprocessing import load_image\n",
    "#from .preprocessing import preprocess_images\n",
    "#from .boxes import assign_prior_boxes\n",
    "\n",
    "class ImageGenerator(object):\n",
    "    \"\"\" Image generator with saturation, brightness, lighting, contrast,\n",
    "    horizontal flip and vertical flip transformations. It supports\n",
    "    bounding boxes coordinates.\n",
    "\n",
    "    TODO:\n",
    "        - Finish preprocess_images method.\n",
    "        - Add random crop method.\n",
    "        - Finish support for not using bounding_boxes.\n",
    "    \"\"\"\n",
    "    \n",
    "    #change ground_truth_data to ground_truth_data\n",
    "    def __init__(self, ground_truth_data, prior_boxes, num_classes,\n",
    "                box_scale_factors,\n",
    "                batch_size, image_size,\n",
    "                train_keys, validation_keys, path_prefix=None,\n",
    "                saturation_var=0.5,\n",
    "                brightness_var=0.5,\n",
    "                contrast_var=0.5,\n",
    "                lighting_std=0.5,\n",
    "                horizontal_flip_probability=0.5,\n",
    "                vertical_flip_probability=0.5,\n",
    "                do_crop=True,\n",
    "                crop_area_range=[0.75, 1.0],\n",
    "                aspect_ratio_range=[3./4., 4./3.]):\n",
    "\n",
    "        self.ground_truth_data = ground_truth_data\n",
    "        self.prior_boxes = prior_boxes\n",
    "        self.num_classes = num_classes\n",
    "        self.box_scale_factors = box_scale_factors\n",
    "        self.batch_size = batch_size\n",
    "        self.path_prefix = path_prefix\n",
    "        self.train_keys = train_keys\n",
    "        self.validation_keys = validation_keys\n",
    "        self.image_size = image_size\n",
    "        self.color_jitter = []\n",
    "        if saturation_var:\n",
    "            self.saturation_var = saturation_var\n",
    "            self.color_jitter.append(self.saturation)\n",
    "        if brightness_var:\n",
    "            self.brightness_var = brightness_var\n",
    "            self.color_jitter.append(self.brightness)\n",
    "        if contrast_var:\n",
    "            self.contrast_var = contrast_var\n",
    "            self.color_jitter.append(self.contrast)\n",
    "        self.lighting_std = lighting_std\n",
    "        self.horizontal_flip_probability = horizontal_flip_probability\n",
    "        self.vertical_flip_probability = vertical_flip_probability\n",
    "        self.do_crop = do_crop\n",
    "        self.crop_area_range = crop_area_range\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "\n",
    "    def _gray_scale(self, image_array):\n",
    "        return image_array.dot([0.299, 0.587, 0.114])\n",
    "\n",
    "    def saturation(self, image_array):\n",
    "        gray_scale = self._gray_scale(image_array)\n",
    "        alpha = 2.0 * np.random.random() * self.brightness_var\n",
    "        alpha = alpha + 1 - self.saturation_var\n",
    "        image_array = alpha * image_array + (1 - alpha) * gray_scale[:, :, None]\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def brightness(self, image_array):\n",
    "        alpha = 2 * np.random.random() * self.brightness_var\n",
    "        alpha = alpha + 1 - self.saturation_var\n",
    "        image_array = alpha * image_array\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def contrast(self, image_array):\n",
    "        gray_scale = (self._gray_scale(image_array).mean() *\n",
    "                        np.ones_like(image_array))\n",
    "        alpha = 2 * np.random.random() * self.contrast_var\n",
    "        alpha = alpha + 1 - self.contrast_var\n",
    "        image_array = image_array * alpha + (1 - alpha) * gray_scale\n",
    "        return np.clip(image_array, 0, 255)\n",
    "\n",
    "    def lighting(self, image_array):\n",
    "        covariance_matrix = np.cov(image_array.reshape(-1,3) /\n",
    "                                    255.0, rowvar=False)\n",
    "        eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)\n",
    "        noise = np.random.randn(3) * self.lighting_std\n",
    "        noise = eigen_vectors.dot(eigen_values * noise) * 255\n",
    "        image_array = image_array + noise\n",
    "        return np.clip(image_array, 0 , 255)\n",
    "\n",
    "    def horizontal_flip(self, image_array, box_corners):\n",
    "        if np.random.random() < self.horizontal_flip_probability:\n",
    "            image_array = image_array[:, ::-1]\n",
    "            box_corners[:, [0, 2]] = 1 - box_corners[:, [2, 0]]\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def vertical_flip(self, image_array, box_corners):\n",
    "        if (np.random.random() < self.vertical_flip_probability):\n",
    "            image_array = image_array[::-1]\n",
    "            box_corners[:, [1, 3]] = 1 - box_corners[:, [3, 1]]\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def transform(self, image_array, box_corners):\n",
    "        shuffle(self.color_jitter)\n",
    "        for jitter in self.color_jitter:\n",
    "            image_array = jitter(image_array)\n",
    "\n",
    "        if self.lighting_std:\n",
    "            image_array = self.lighting(image_array)\n",
    "\n",
    "        if self.horizontal_flip_probability > 0:\n",
    "            image_array, box_corners = self.horizontal_flip(image_array,\n",
    "                                                            box_corners)\n",
    "\n",
    "        if self.vertical_flip_probability > 0:\n",
    "            image_array, box_corners = self.vertical_flip(image_array,\n",
    "                                                            box_corners)\n",
    "\n",
    "        return image_array, box_corners\n",
    "\n",
    "    def flow(self, mode='train'):\n",
    "            while True:\n",
    "                if mode =='train':\n",
    "                    print('Train mode set')\n",
    "                    shuffle(self.train_keys)\n",
    "                    keys = self.train_keys\n",
    "                elif mode == 'val' or  mode == 'demo':\n",
    "                    print('val/demo mode set')\n",
    "                    shuffle(self.validation_keys)\n",
    "                    keys = self.validation_keys\n",
    "                else:\n",
    "                    raise Exception('invalid mode: %s' % mode)\n",
    "\n",
    "                inputs = []\n",
    "                targets = []\n",
    "                for key in keys:\n",
    "                    print('key_mun', key)\n",
    "                    image_path = self.path_prefix + key\n",
    "                    image_array = load_image(image_path, self.image_size)\n",
    "                    print('image_array_before', image_array)\n",
    "                    box_corners = self.ground_truth_data[key].copy()\n",
    "                    \n",
    "                    if mode == 'train' or mode == 'demo':\n",
    "                        image_array, box_corners = self.transform(image_array,\n",
    "                                                                box_corners)\n",
    "                    print('box_corners',box_corners)\n",
    "                    box_corners = assign_prior_boxes(self.prior_boxes,\n",
    "                                                    box_corners,\n",
    "                                                    self.num_classes,\n",
    "                                                    self.box_scale_factors)\n",
    "                    print('box_corners_with anchors',box_corners)\n",
    "                    inputs.append(image_array)\n",
    "                    print('inputs', inputs)\n",
    "                    targets.append(box_corners)\n",
    "                    print('targets', targets)\n",
    "                    if len(targets) == self.batch_size:\n",
    "                        inputs = np.asarray(inputs)\n",
    "                        targets = np.asarray(targets)\n",
    "                        if mode == 'train' or mode == 'val':\n",
    "                            inputs = preprocess_images(inputs)\n",
    "                            yield self._wrap_in_dictionary(inputs, targets)\n",
    "                        if mode == 'demo':\n",
    "                            yield self._wrap_in_dictionary(inputs, targets)\n",
    "                        inputs = []\n",
    "                        targets = []\n",
    "\n",
    "    def _wrap_in_dictionary(self, image_array, targets):\n",
    "        return [{'input_1':image_array},\n",
    "                {'predictions':targets}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt = dataset\n",
    "keys = sorted(gt.keys())\n",
    "shuffle(keys)\n",
    "num_train = int(round(0.8 * len(keys)))\n",
    "train_keys = keys[:num_train]\n",
    "val_keys = keys[num_train:]\n",
    "num_val = len(val_keys)\n",
    " \n",
    " \n",
    "print('train_keys',len(train_keys))\n",
    "print('val_keys',len(val_keys))\n",
    "\n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_data = gt\n",
    "\n",
    "train_keys, validation_keys = split_data(ground_truth_data, training_ratio=.8)\n",
    "\n",
    "#print('ground_truth_data sample',ground_truth_data )\n",
    "#print('train_keys', train_keys)\n",
    "#print('validation_keys', len(validation_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "# instantiating model\n",
    "from keras import metrics\n",
    "model = SSD300(image_shape, num_classes, weights_path, frozen_layers)\n",
    "multibox_loss = MultiboxLoss(num_classes, neg_pos_ratio=2.0).compute_loss\n",
    "model.compile(optimizer, loss=multibox_loss, metrics=['acc'])#,metrics.mae, metrics.categorical_accuracy])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting parameters for data augmentation generator\n",
    "batch_size = 1\n",
    "prior_boxes = create_prior_boxes(model)\n",
    " \n",
    "image_generator = ImageGenerator(ground_truth_data,\n",
    "                                 prior_boxes,\n",
    "                                 num_classes,\n",
    "                                 box_scale_factors,\n",
    "                                 batch_size,\n",
    "                                 image_shape[0:2],\n",
    "                                 train_keys, validation_keys,\n",
    "                                 image_prefix,\n",
    "                                 vertical_flip_probability=0,\n",
    "                                 horizontal_flip_probability=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################################################\n",
    "## Testing generator\n",
    "\n",
    " \n",
    "generated_data = next(image_generator.flow(mode='val'))\n",
    "generated_input = generated_data[0]['input_1']\n",
    "#print('generated_data', generated_data)\n",
    "generated_output = generated_data[1]['predictions']\n",
    "generated_image = np.squeeze(generated_input[0]).astype('uint8')\n",
    "validation_image_name = image_prefix + validation_keys[0]\n",
    "original_image = load_image(validation_image_name, input_shape)\n",
    "plot_images(original_image, generated_image)\n",
    "\n",
    "#########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_decoder={0: 'background', 1: 'car', 2: 'pedestrian', 2: 'truck'}\n",
    "generated_encoded_boxes = np.squeeze(generated_output)\n",
    "generated_boxes = decode_boxes(generated_encoded_boxes, prior_boxes,\n",
    "                                                  box_scale_factors)\n",
    "positive_mask = generated_boxes[:, 4] != 1\n",
    "generated_positive_boxes = generated_boxes[positive_mask]\n",
    "draw_image_boxes(generated_positive_boxes, generated_image, class_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiating callbacks\n",
    "learning_rate_schedule = LearningRateScheduler(scheduler)\n",
    "model_names = (trained_models_filename)\n",
    "model_checkpoint = ModelCheckpoint(model_names,\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False,\n",
    "                                   save_weights_only=False)\n",
    "\n",
    "model_tensorboard = TensorBoard(log_dir='./logs/200-epoch', \n",
    "                                         histogram_freq=10,\n",
    "                                         #batch_size = 8,\n",
    "                                         write_graph=True,\n",
    "                                         write_images=True,\n",
    "                                          \n",
    "                                         #write_grads=True,\n",
    "                                         embeddings_freq =5,\n",
    "                                         embeddings_layer_names = 'conv3_3, conv4_3' \n",
    "                                 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "# training model with real-time data augmentation\n",
    "model.fit_generator(image_generator.flow(mode='train'),\n",
    "                    steps_per_epoch=int(len(train_keys) / batch_size),\n",
    "                    epochs = num_epochs, verbose = 1,\n",
    "                    callbacks=[model_checkpoint, learning_rate_schedule],\n",
    "                    validation_data=image_generator.flow(mode='val'),\n",
    "                    validation_steps=int(len(validation_keys) / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
